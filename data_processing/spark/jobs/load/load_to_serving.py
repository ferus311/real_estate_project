"""
Load d·ªØ li·ªáu v√†o Serving Layer (PostgreSQL) cho Web API
"""

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col,
    lit,
    current_timestamp,
    when,
    coalesce,
    row_number,
    desc,
    substring,
    length,
)
from pyspark.sql.window import Window
import sys
import os
from datetime import datetime
import argparse
from dotenv import load_dotenv

load_dotenv()

# Th√™m th∆∞ m·ª•c g·ªëc v√†o sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.append(project_root)

from common.utils.date_utils import get_date_format, get_hdfs_path
from common.utils.hdfs_utils import check_hdfs_path_exists
from common.utils.logging_utils import SparkJobLogger
from common.config.spark_config import create_spark_session
from common.utils.duplicate_detection import apply_load_deduplication


def load_to_serving_layer(
    spark: SparkSession,
    input_date=None,
    property_type="house",
    postgres_config=None,
) -> None:
    """
    Load d·ªØ li·ªáu t·ª´ Gold layer v√†o PostgreSQL serving layer

    Args:
        spark: SparkSession
        input_date: Ng√†y x·ª≠ l√Ω, ƒë·ªãnh d·∫°ng "YYYY-MM-DD"
        property_type: Lo·∫°i b·∫•t ƒë·ªông s·∫£n ("house", "other", "all")
        postgres_config: C·∫•u h√¨nh PostgreSQL connection
    """
    logger = SparkJobLogger("load_to_serving_layer")
    logger.start_job(
        {
            "input_date": input_date,
            "property_type": property_type,
        }
    )

    if input_date is None:
        input_date = get_date_format()

    if postgres_config is None:
        postgres_config = {
            "url": f"jdbc:postgresql://{os.getenv('POSTGRES_HOST', 'db')}:{os.getenv('POSTGRES_PORT', '5432')}/{os.getenv('POSTGRES_DB', 'realestate')}",
            "user": os.getenv("POSTGRES_USER", "postgres"),
            "password": os.getenv("POSTGRES_PASSWORD", "password"),
            "driver": "org.postgresql.Driver",
        }

    try:
        # 1. Extract t·ª´ Gold layer
        gold_df = extract_from_gold(spark, input_date, property_type, logger)

        # 2. Transform cho serving needs
        serving_df = transform_for_serving(gold_df, logger)

        # 3. Apply deduplication
        initial_count = serving_df.count()
        if initial_count == 0:
            logger.logger.info("üìä No data to process")
            logger.end_job()
            return

        logger.logger.info(f"üîç Applying deduplication to {initial_count:,} records...")
        # S·ª≠ d·ª•ng window 90 ng√†y ƒë·ªÉ ph√°t hi·ªán tr√πng l·∫∑p t·ªët h∆°n
        deduplicated_df = apply_load_deduplication(
            serving_df, postgres_config, days_window=90
        )
        final_count = deduplicated_df.count()

        logger.logger.info(
            f"üìä Deduplication result: {final_count:,}/{initial_count:,} records"
        )

        # 4. Load v√†o PostgreSQL
        if final_count > 0:
            load_to_postgres(deduplicated_df, postgres_config, logger)

        logger.logger.info("‚úÖ Load to serving completed!")
        logger.end_job()

    except Exception as e:
        logger.log_error("‚ùå Load to serving layer failed", e)
        raise


def extract_from_gold(spark: SparkSession, input_date: str, property_type: str, logger):
    """Extract d·ªØ li·ªáu t·ª´ Gold layer"""

    if property_type == "all":
        property_types = ["house", "other"]
    else:
        property_types = [property_type]

    all_dfs = []

    for ptype in property_types:
        date_formatted = input_date.replace("-", "/")
        gold_path = f"/data/realestate/processed/gold/unified/{ptype}/{date_formatted}/unified_*.parquet"

        try:
            df = spark.read.parquet(gold_path)
            logger.logger.info(
                f"üìä Loaded {df.count():,} records from Gold layer for {ptype}"
            )
            all_dfs.append(df)
        except Exception as e:
            logger.logger.warning(f"‚ö†Ô∏è No Gold data found for {ptype}: {e}")

    if not all_dfs:
        raise FileNotFoundError(f"No Gold data found for date {input_date}")

    # Union all dataframes
    combined_df = all_dfs[0]
    for df in all_dfs[1:]:
        combined_df = combined_df.union(df)

    return combined_df


def transform_for_serving(gold_df: DataFrame, logger):
    """Transform d·ªØ li·ªáu t·ª´ Gold layer cho PostgreSQL serving layer

    √Åp d·ª•ng gi·ªõi h·∫°n k√Ω t·ª± cho c√°c tr∆∞·ªùng text ƒë·ªÉ tr√°nh l·ªói database:
    - title: t·ªëi ƒëa 500 k√Ω t·ª±
    - description: t·ªëi ƒëa 2000 k√Ω t·ª±
    - location: t·ªëi ƒëa 500 k√Ω t·ª±
    - province: t·ªëi ƒëa 100 k√Ω t·ª±
    - district: t·ªëi ƒëa 100 k√Ω t·ª±
    - ward: t·ªëi ƒëa 200 k√Ω t·ª±
    - street: t·ªëi ƒëa 300 k√Ω t·ª±
    - house_direction: t·ªëi ƒëa 50 k√Ω t·ª±
    - legal_status: t·ªëi ƒëa 100 k√Ω t·ª±
    - interior: t·ªëi ƒëa 100 k√Ω t·ª±
    - house_type: t·ªëi ƒëa 100 k√Ω t·ª±
    """

    logger.logger.info("üîÑ Transforming data for serving layer...")

    # Complete mapping t·ª´ Gold schema sang PostgreSQL schema
    serving_df = gold_df.select(
        # Primary identifiers
        col("id"),
        col("url"),
        col("source"),
        # Basic property information
        col("title"),
        col("description"),
        col("location"),
        col("data_type"),
        # Location information
        # Gi·ªõi h·∫°n province t·ªëi ƒëa 100 k√Ω t·ª±
        when(col("province").isNull(), col("province"))
        .otherwise(substring(col("province"), 1, 100))
        .alias("province"),
        # Gi·ªõi h·∫°n district t·ªëi ƒëa 100 k√Ω t·ª±
        when(col("district").isNull(), col("district"))
        .otherwise(substring(col("district"), 1, 100))
        .alias("district"),
        # Gi·ªõi h·∫°n ward t·ªëi ƒëa 200 k√Ω t·ª± ƒë·ªÉ tr√°nh l·ªói database
        when(col("ward").isNull(), col("ward"))
        .otherwise(substring(col("ward"), 1, 200))
        .alias("ward"),
        # Gi·ªõi h·∫°n street t·ªëi ƒëa 300 k√Ω t·ª±
        when(col("street").isNull(), col("street"))
        .otherwise(substring(col("street"), 1, 200))
        .alias("street"),
        # Location IDs
        col("province_id").cast("int"),
        col("district_id").cast("int"),
        col("ward_id").cast("int"),
        col("street_id").cast("int"),
        # Geographic coordinates
        col("latitude").cast("double"),
        col("longitude").cast("double"),
        # Core metrics
        col("price").cast("bigint"),
        col("area").cast("double"),
        col("price_per_m2").cast("bigint"),
        # Property details
        col("bedroom").cast("double"),
        col("bathroom").cast("double"),
        col("floor_count").cast("int"),
        # Dimensions
        col("width").cast("double"),
        col("length").cast("double"),
        col("living_size").cast("double"),
        col("facade_width").cast("double"),
        col("road_width").cast("double"),
        # Property characteristics
        # Gi·ªõi h·∫°n house_direction t·ªëi ƒëa 50 k√Ω t·ª±
        when(col("house_direction").isNull(), col("house_direction"))
        .otherwise(substring(col("house_direction"), 1, 50))
        .alias("house_direction"),
        col("house_direction_code").cast("int"),
        # Gi·ªõi h·∫°n legal_status t·ªëi ƒëa 100 k√Ω t·ª±
        when(col("legal_status").isNull(), col("legal_status"))
        .otherwise(substring(col("legal_status"), 1, 100))
        .alias("legal_status"),
        col("legal_status_code").cast("int"),
        # Gi·ªõi h·∫°n interior t·ªëi ƒëa 100 k√Ω t·ª±
        when(col("interior").isNull(), col("interior"))
        .otherwise(substring(col("interior"), 1, 100))
        .alias("interior"),
        col("interior_code").cast("int"),
        # Gi·ªõi h·∫°n house_type t·ªëi ƒëa 100 k√Ω t·ª±
        when(col("house_type").isNull(), col("house_type"))
        .otherwise(substring(col("house_type"), 1, 100))
        .alias("house_type"),
        col("house_type_code").cast("int"),
        # Timestamps
        col("posted_date").cast("timestamp"),
        col("crawl_timestamp").cast("timestamp"),
        col("processing_timestamp").cast("timestamp"),
        # Data quality
        col("data_quality_score").cast("double"),
        col("processing_id"),
        # Serving metadata
        current_timestamp().alias("created_at"),
        current_timestamp().alias("updated_at"),
    ).filter(
        col("id").isNotNull()
        & col("title").isNotNull()
        & col("source").isNotNull()
        & (col("price").isNull() | (col("price") >= 0))
        & (col("area").isNull() | (col("area") > 0))
        & (
            (col("latitude").isNull() & col("longitude").isNull())
            | (col("latitude").between(-90, 90) & col("longitude").between(-180, 180))
        )
    )

    # Dedup by ID with data quality score
    window_spec = Window.partitionBy("id").orderBy(
        desc("data_quality_score"), desc("processing_timestamp"), desc("updated_at")
    )

    serving_df = (
        serving_df.withColumn("row_num", row_number().over(window_spec))
        .filter(col("row_num") == 1)
        .drop("row_num")
    )

    original_count = gold_df.count()
    final_count = serving_df.count()
    logger.logger.info(
        f"üìä Final result: {final_count:,}/{original_count:,} records for serving"
    )

    return serving_df


def load_to_postgres(serving_df: DataFrame, postgres_config: dict, logger):
    """
    Load d·ªØ li·ªáu v√†o PostgreSQL s·ª≠ d·ª•ng staging table v√† upsert ƒë·ªÉ tr√°nh l·ªói tr√πng l·∫∑p

    Quy tr√¨nh c·∫£i thi·∫øn:
    1. T·∫°o b·∫£ng staging (properties_staging) v·ªõi c·∫•u tr√∫c gi·ªëng b·∫£ng ch√≠nh
    2. Ghi d·ªØ li·ªáu v√†o b·∫£ng staging
    3. Th·ª±c hi·ªán UPSERT t·ª´ staging v√†o b·∫£ng ch√≠nh v·ªõi logic c·∫≠p nh·∫≠t th√¥ng minh:
       - Ch·ªâ c·∫≠p nh·∫≠t khi b·∫£n ghi m·ªõi c√≥ ch·∫•t l∆∞·ª£ng t·ªët h∆°n (data_quality_score cao h∆°n)
       - Ho·∫∑c c√πng ch·∫•t l∆∞·ª£ng nh∆∞ng m·ªõi h∆°n (processing_timestamp g·∫ßn ƒë√¢y h∆°n)
    4. X√≥a b·∫£ng staging sau khi ho√†n th√†nh
    5. X·ª≠ l√Ω l·ªói v√† rollback to√†n di·ªán
    """

    if postgres_config is None:
        postgres_config = {
            "url": f"jdbc:postgresql://{os.getenv('POSTGRES_HOST', 'db')}:{os.getenv('POSTGRES_PORT', '5432')}/{os.getenv('POSTGRES_DB', 'realestate')}",
            "user": os.getenv("POSTGRES_USER", "postgres"),
            "password": os.getenv("POSTGRES_PASSWORD", "password"),
            "driver": "org.postgresql.Driver",
        }

    serving_df = serving_df.coalesce(4)

    # Parse PostgreSQL connection details
    url_parts = postgres_config["url"].replace("jdbc:postgresql://", "").split(":")
    host = url_parts[0]
    port = url_parts[1].split("/")[0]
    database = url_parts[1].split("/")[1]

    # Base JDBC options
    base_options = {
        "url": postgres_config["url"],
        "user": postgres_config["user"],
        "password": postgres_config["password"],
        "driver": postgres_config["driver"],
        "batchsize": "5000",
        "numPartitions": "4",
        "isolationLevel": "READ_COMMITTED",
        "stringtype": "unspecified",
    }

    try:
        import psycopg2
        from psycopg2 import sql

        total_records = serving_df.count()
        logger.logger.info(
            f"üîÑ Loading {total_records:,} records to PostgreSQL using improved staging table approach..."
        )

        # B∆∞·ªõc 1: T·∫°o v√† ghi v√†o b·∫£ng t·∫°m (staging)
        logger.logger.info("B∆∞·ªõc 1: Chu·∫©n b·ªã v√† ghi v√†o b·∫£ng staging...")

        conn = None
        cursor = None
        try:
            # K·∫øt n·ªëi ƒë·ªÉ chu·∫©n b·ªã b·∫£ng t·∫°m v·ªõi timeout
            conn = psycopg2.connect(
                host=host,
                port=port,
                database=database,
                user=postgres_config["user"],
                password=postgres_config["password"],
                connect_timeout=30,  # Th√™m timeout ƒë·ªÉ tr√°nh treo khi k·∫øt n·ªëi
            )
            cursor = conn.cursor()

            # T·∫°o b·∫£ng t·∫°m n·∫øu ch∆∞a t·ªìn t·∫°i ho·∫∑c x√≥a d·ªØ li·ªáu n·∫øu ƒë√£ t·ªìn t·∫°i
            cursor.execute("DROP TABLE IF EXISTS properties_staging")

            # T·∫°o b·∫£ng t·∫°m v·ªõi c√πng c·∫•u tr√∫c nh∆∞ b·∫£ng ch√≠nh
            cursor.execute(
                """
            CREATE TABLE properties_staging (LIKE properties INCLUDING ALL)
            """
            )
            conn.commit()
            logger.logger.info("‚úÖ B·∫£ng staging ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng")

        except Exception as e:
            logger.logger.error(f"‚ùå L·ªói khi t·∫°o b·∫£ng staging: {str(e)}")
            if conn:
                try:
                    conn.rollback()
                except:
                    pass
            raise
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()

        # Ghi d·ªØ li·ªáu v√†o b·∫£ng t·∫°m
        try:
            staging_options = base_options.copy()
            staging_options["dbtable"] = "properties_staging"

            # Th√™m th√¥ng s·ªë ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t v√† gi·∫£m l·ªói
            staging_options["numPartitions"] = "8"  # TƒÉng s·ªë l∆∞·ª£ng partition
            staging_options["batchsize"] = "1000"  # Gi·∫£m k√≠ch th∆∞·ªõc batch ƒë·ªÉ tr√°nh OOM

            # Ghi d·ªØ li·ªáu v√†o staging table
            serving_df.write.format("jdbc").options(**staging_options).mode(
                "append"
            ).save()
            logger.logger.info(f"‚úÖ ƒê√£ ghi {total_records:,} b·∫£n ghi v√†o b·∫£ng staging")

        except Exception as e:
            logger.logger.error(f"‚ùå L·ªói khi ghi d·ªØ li·ªáu v√†o b·∫£ng staging: {str(e)}")
            # Kh√¥i ph·ª•c b·∫±ng c√°ch x√≥a b·∫£ng staging
            try:
                conn = psycopg2.connect(
                    host=host,
                    port=port,
                    database=database,
                    user=postgres_config["user"],
                    password=postgres_config["password"],
                )
                cursor = conn.cursor()
                cursor.execute("DROP TABLE IF EXISTS properties_staging")
                conn.commit()
                cursor.close()
                conn.close()
            except:
                pass  # B·ªè qua l·ªói trong qu√° tr√¨nh d·ªçn d·∫πp
            raise

        # B∆∞·ªõc 2: Th·ª±c hi·ªán UPSERT t·ª´ b·∫£ng t·∫°m sang b·∫£ng ch√≠nh
        logger.logger.info("B∆∞·ªõc 2: Th·ª±c hi·ªán UPSERT t·ª´ b·∫£ng staging v√†o b·∫£ng ch√≠nh...")

        conn = None
        cursor = None
        try:
            conn = psycopg2.connect(
                host=host,
                port=port,
                database=database,
                user=postgres_config["user"],
                password=postgres_config["password"],
                connect_timeout=30,
            )
            # Ch·∫ø ƒë·ªô transaction t·ª± ƒë·ªông
            conn.autocommit = False
            cursor = conn.cursor()

            # Thi·∫øt l·∫≠p statement timeout l·ªõn h∆°n
            cursor.execute("SET statement_timeout = '1800000'")  # 30 ph√∫t

            # L·∫•y th·ªëng k√™ tr∆∞·ªõc khi upsert
            cursor.execute("SELECT COUNT(*) FROM properties")
            count_before = cursor.fetchone()[0]

            # Th·ª±c hi·ªán UPSERT v·ªõi logic c·∫≠p nh·∫≠t c·∫£i ti·∫øn
            # S·ª≠ d·ª•ng ki·∫øn tr√∫c UPSERT theo khuy·∫øn ngh·ªã c·ªßa PostgreSQL
            upsert_query = """
            INSERT INTO properties
            SELECT * FROM properties_staging
            ON CONFLICT (id)
            DO UPDATE SET
                -- C·∫≠p nh·∫≠t t·∫•t c·∫£ c√°c tr∆∞·ªùng
                title = EXCLUDED.title,
                description = EXCLUDED.description,
                url = EXCLUDED.url,
                source = EXCLUDED.source,
                location = EXCLUDED.location,
                data_type = EXCLUDED.data_type,
                province = EXCLUDED.province,
                district = EXCLUDED.district,
                ward = EXCLUDED.ward,
                street = EXCLUDED.street,
                province_id = EXCLUDED.province_id,
                district_id = EXCLUDED.district_id,
                ward_id = EXCLUDED.ward_id,
                street_id = EXCLUDED.street_id,
                latitude = EXCLUDED.latitude,
                longitude = EXCLUDED.longitude,
                price = EXCLUDED.price,
                area = EXCLUDED.area,
                price_per_m2 = EXCLUDED.price_per_m2,
                bedroom = EXCLUDED.bedroom,
                bathroom = EXCLUDED.bathroom,
                floor_count = EXCLUDED.floor_count,
                width = EXCLUDED.width,
                length = EXCLUDED.length,
                living_size = EXCLUDED.living_size,
                facade_width = EXCLUDED.facade_width,
                road_width = EXCLUDED.road_width,
                house_direction = EXCLUDED.house_direction,
                house_direction_code = EXCLUDED.house_direction_code,
                legal_status = EXCLUDED.legal_status,
                legal_status_code = EXCLUDED.legal_status_code,
                interior = EXCLUDED.interior,
                interior_code = EXCLUDED.interior_code,
                house_type = EXCLUDED.house_type,
                house_type_code = EXCLUDED.house_type_code,
                posted_date = EXCLUDED.posted_date,
                crawl_timestamp = EXCLUDED.crawl_timestamp,
                processing_timestamp = EXCLUDED.processing_timestamp,
                data_quality_score = EXCLUDED.data_quality_score,
                processing_id = EXCLUDED.processing_id,
                updated_at = CURRENT_TIMESTAMP
            WHERE
                -- Ch·ªâ c·∫≠p nh·∫≠t khi d·ªØ li·ªáu m·ªõi t·ªët h∆°n ho·∫∑c m·ªõi h∆°n
                EXCLUDED.data_quality_score > properties.data_quality_score
                OR (EXCLUDED.data_quality_score >= properties.data_quality_score
                    AND EXCLUDED.processing_timestamp > properties.processing_timestamp)
            """
            cursor.execute(upsert_query)

            # L·∫•y k·∫øt qu·∫£ sau khi upsert
            cursor.execute("SELECT COUNT(*) FROM properties")
            count_after = cursor.fetchone()[0]

            # T√≠nh to√°n s·ªë b·∫£n ghi ƒë√£ th√™m m·ªõi
            new_records = count_after - count_before

            # T√≠nh to√°n s·ªë b·∫£n ghi ƒë√£ c·∫≠p nh·∫≠t
            cursor.execute("SELECT COUNT(*) FROM properties_staging")
            staging_count = cursor.fetchone()[0]
            updated_records = staging_count - new_records

            # Commit transaction khi t·∫•t c·∫£ ƒë√£ th√†nh c√¥ng
            conn.commit()

            logger.logger.info(f"üìä K·∫øt qu·∫£ UPSERT:")
            logger.logger.info(f"   ‚ûï ƒê√£ th√™m m·ªõi: {new_records:,} b·∫£n ghi")
            logger.logger.info(f"   üîÑ ƒê√£ c·∫≠p nh·∫≠t: {updated_records:,} b·∫£n ghi")
            logger.logger.info(f"   üìä T·ªïng b·∫£n ghi hi·ªán t·∫°i: {count_after:,}")

        except Exception as e:
            if conn:
                try:
                    conn.rollback()
                    logger.logger.info("‚úÖ Transaction ƒë√£ ƒë∆∞·ª£c rollback sau l·ªói")
                except:
                    logger.logger.error("‚ùå Kh√¥ng th·ªÉ rollback transaction sau l·ªói")
            logger.logger.error(f"‚ùå L·ªói khi th·ª±c hi·ªán UPSERT: {str(e)}")
            raise
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()

        # B∆∞·ªõc 3: D·ªçn d·∫πp - x√≥a b·∫£ng t·∫°m
        logger.logger.info("B∆∞·ªõc 3: D·ªçn d·∫πp b·∫£ng t·∫°m...")

        try:
            conn = psycopg2.connect(
                host=host,
                port=port,
                database=database,
                user=postgres_config["user"],
                password=postgres_config["password"],
            )
            cursor = conn.cursor()
            cursor.execute("DROP TABLE IF EXISTS properties_staging")
            conn.commit()
            cursor.close()
            conn.close()
            logger.logger.info("‚úÖ ƒê√£ x√≥a b·∫£ng staging")

        except Exception as e:
            logger.logger.error(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng th·ªÉ x√≥a b·∫£ng staging: {str(e)}")
            # Ti·∫øp t·ª•c th·ª±c thi ngay c·∫£ khi qu√° tr√¨nh d·ªçn d·∫πp th·∫•t b·∫°i

        logger.logger.info(f"‚úÖ Ho√†n th√†nh qu√° tr√¨nh load d·ªØ li·ªáu v√†o PostgreSQL")
        return True

    except Exception as e:
        logger.logger.error(f"‚ùå PostgreSQL loading failed: {str(e)}")
        # In traceback ƒë·ªÉ debug chi ti·∫øt h∆°n
        import traceback

        logger.logger.error(traceback.format_exc())
        # M·ªçi l·ªói kh√°c kh√¥ng ƒë∆∞·ª£c x·ª≠ l√Ω s·∫Ω ƒë∆∞·ª£c chuy·ªÉn l√™n c·∫•p cao h∆°n
        raise


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="Load Data to Serving Layer (PostgreSQL)"
    )
    parser.add_argument("--date", type=str, help="Processing date in YYYY-MM-DD format")
    parser.add_argument(
        "--property-type", type=str, default="house", choices=["house", "other", "all"]
    )
    parser.add_argument(
        "--postgres-host", type=str, default=os.getenv("POSTGRES_HOST", "localhost")
    )
    parser.add_argument(
        "--postgres-port", type=str, default=os.getenv("POSTGRES_PORT", "5432")
    )
    parser.add_argument(
        "--postgres-db", type=str, default=os.getenv("POSTGRES_DB", "realestate")
    )
    parser.add_argument(
        "--postgres-user", type=str, default=os.getenv("POSTGRES_USER", "postgres")
    )
    parser.add_argument(
        "--postgres-password",
        type=str,
        default=os.getenv("POSTGRES_PASSWORD", "password"),
    )

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    postgres_config = {
        "url": f"jdbc:postgresql://{args.postgres_host}:{args.postgres_port}/{args.postgres_db}",
        "user": args.postgres_user,
        "password": args.postgres_password,
        "driver": "org.postgresql.Driver",
    }

    spark = create_spark_session(
        "Load Data to Serving Layer",
        config={
            "spark.jars": "/opt/bitnami/spark/jars/postgresql-42.7.0.jar",
            "spark.sql.adaptive.enabled": "true",
            "spark.sql.adaptive.coalescePartitions.enabled": "true",
        },
    )

    try:
        load_to_serving_layer(spark, args.date, args.property_type, postgres_config)
        print("‚úÖ Serving layer load completed successfully!")
    finally:
        spark.stop()
