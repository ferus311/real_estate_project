"""
Load d·ªØ li·ªáu v√†o Serving Layer (PostgreSQL) cho Web API
"""

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, lit, current_timestamp, when, coalesce
import sys
import os
from datetime import datetime
import argparse

# Th√™m th∆∞ m·ª•c g·ªëc v√†o sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.append(project_root)

from common.utils.date_utils import get_date_format, get_hdfs_path
from common.utils.hdfs_utils import check_hdfs_path_exists
from common.utils.logging_utils import SparkJobLogger
from common.config.spark_config import create_spark_session


def load_to_serving_layer(
    spark: SparkSession,
    input_date=None,
    property_type="house",
    postgres_config=None,
) -> None:
    """
    Load d·ªØ li·ªáu t·ª´ Gold layer v√†o PostgreSQL serving layer

    Args:
        spark: SparkSession
        input_date: Ng√†y x·ª≠ l√Ω, ƒë·ªãnh d·∫°ng "YYYY-MM-DD"
        property_type: Lo·∫°i b·∫•t ƒë·ªông s·∫£n ("house", "other", "all")
        postgres_config: C·∫•u h√¨nh PostgreSQL connection
    """
    logger = SparkJobLogger("load_to_serving_layer")
    logger.start_job(
        {
            "input_date": input_date,
            "property_type": property_type,
        }
    )

    # X√°c ƒë·ªãnh ng√†y x·ª≠ l√Ω
    if input_date is None:
        input_date = get_date_format()

    # Default PostgreSQL config
    if postgres_config is None:
        postgres_config = {
            "url": "jdbc:postgresql://realestate-postgres:5432/realestate",
            "user": "postgres",
            "password": "password",
            "driver": "org.postgresql.Driver",
        }

    try:
        # 1. Extract t·ª´ Gold layer
        gold_df = extract_from_gold(spark, input_date, property_type, logger)

        # 2. Transform cho serving needs
        serving_df = transform_for_serving(gold_df, logger)

        # 3. Load v√†o PostgreSQL
        load_to_postgres(serving_df, postgres_config, logger)

        logger.logger.info("‚úÖ Load to serving layer completed successfully!")
        logger.end_job()

    except Exception as e:
        logger.log_error("‚ùå Load to serving layer failed", e)
        raise


def extract_from_gold(spark: SparkSession, input_date: str, property_type: str, logger):
    """Extract d·ªØ li·ªáu t·ª´ Gold layer"""

    if property_type == "all":
        property_types = ["house", "other"]
    else:
        property_types = [property_type]

    all_dfs = []

    for ptype in property_types:
        # ƒê·ªçc t·ª´ unified data trong Gold layer
        gold_path = get_hdfs_path(
            "/data/realestate/processed/gold/unified", ptype, input_date
        )

        try:
            df = spark.read.parquet(gold_path)
            logger.logger.info(
                f"üìä Loaded {df.count():,} records from Gold layer for {ptype}"
            )
            all_dfs.append(df)
        except Exception as e:
            logger.logger.warning(f"‚ö†Ô∏è No Gold data found for {ptype}: {e}")

    if not all_dfs:
        raise FileNotFoundError(f"No Gold data found for date {input_date}")

    # Union all dataframes
    combined_df = all_dfs[0]
    for df in all_dfs[1:]:
        combined_df = combined_df.union(df)

    return combined_df


def transform_for_serving(gold_df: DataFrame, logger):
    """Transform d·ªØ li·ªáu cho serving layer"""

    # Debug: Ki·ªÉm tra schema c·ªßa Gold layer
    logger.logger.info("üîç Gold layer schema:")
    for field in gold_df.schema.fields:
        logger.logger.info(f"  - {field.name}: {field.dataType}")

    # Select v√† transform fields cho web serving - map t·ª´ Gold schema
    serving_df = gold_df.select(
        # Core property fields
        col("id").alias("property_id"),
        col("title"),
        col("price").cast("bigint"),
        col("area").cast("real"),
        col("bedroom").cast("int").alias("bedrooms"),  # Map bedroom -> bedrooms
        col("bathroom").cast("int").alias("bathrooms"),  # Map bathroom -> bathrooms
        col("description"),
        # Location fields
        col("province"),
        col("district"),
        col("ward"),
        col("location").alias("address"),  # Map location -> address
        col("latitude").cast("double"),
        col("longitude").cast("double"),
        # Source fields
        col("source"),
        col("url"),
        # Metadata - s·ª≠ d·ª•ng processing_timestamp t·ª´ Gold n·∫øu c√≥
        coalesce(col("processing_timestamp"), current_timestamp()).alias("created_at"),
        current_timestamp().alias("updated_at"),
        lit(datetime.now().strftime("%Y-%m-%d")).alias("processing_date"),
    ).filter(
        # Basic data quality filters for serving
        col("price").isNotNull()
        & col("area").isNotNull()
        & col("title").isNotNull()
        & (col("price") > 0)
        & (col("area") > 0)
    )

    logger.logger.info(f"üìä Transformed {serving_df.count():,} records for serving")
    return serving_df


def load_to_postgres(serving_df: DataFrame, postgres_config: dict, logger):
    """Load d·ªØ li·ªáu v√†o PostgreSQL v·ªõi upsert logic"""

    # PostgreSQL c√≥ UNIQUE constraint tr√™n (url, source), n√™n s·∫Ω conflict khi c√≥ duplicate
    # C√°ch 1: Th·ª≠ append tr∆∞·ªõc, n·∫øu failed th√¨ overwrite staging table
    try:
        serving_df.write.format("jdbc").option("url", postgres_config["url"]).option(
            "dbtable", "properties"
        ).option("user", postgres_config["user"]).option(
            "password", postgres_config["password"]
        ).option(
            "driver", postgres_config["driver"]
        ).option(
            "batchsize", "5000"
        ).option(
            "numPartitions", "4"
        ).mode(
            "append"
        ).save()

        logger.logger.info("‚úÖ Successfully loaded data to PostgreSQL")

    except Exception as e:
        # N·∫øu append failed (c√≥ th·ªÉ do duplicates), th·ª≠ load v√†o staging table
        logger.logger.warning(f"‚ö†Ô∏è Append failed, trying staging table approach: {e}")

        try:
            # Load v√†o staging table v·ªõi overwrite
            serving_df.write.format("jdbc").option(
                "url", postgres_config["url"]
            ).option("dbtable", "properties_staging").option(
                "user", postgres_config["user"]
            ).option(
                "password", postgres_config["password"]
            ).option(
                "driver", postgres_config["driver"]
            ).option(
                "batchsize", "5000"
            ).option(
                "numPartitions", "4"
            ).mode(
                "overwrite"
            ).save()

            logger.logger.info(
                "‚úÖ Successfully loaded data to PostgreSQL staging table"
            )

        except Exception as e2:
            logger.logger.error(f"‚ùå Both append and staging failed: {e2}")
            raise


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="Load Data to Serving Layer (PostgreSQL)"
    )
    parser.add_argument("--date", type=str, help="Processing date in YYYY-MM-DD format")
    parser.add_argument(
        "--property-type",
        type=str,
        default="house",
        choices=["house", "other", "all"],
        help="Property type",
    )
    parser.add_argument(
        "--postgres-host", type=str, default="postgres", help="PostgreSQL host"
    )
    parser.add_argument(
        "--postgres-port", type=str, default="5432", help="PostgreSQL port"
    )
    parser.add_argument(
        "--postgres-db", type=str, default="realestate", help="PostgreSQL database"
    )
    parser.add_argument(
        "--postgres-user", type=str, default="postgres", help="PostgreSQL user"
    )
    parser.add_argument(
        "--postgres-password", type=str, default="password", help="PostgreSQL password"
    )

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    # C·∫•u h√¨nh PostgreSQL t·ª´ arguments
    postgres_config = {
        "url": f"jdbc:postgresql://{args.postgres_host}:{args.postgres_port}/{args.postgres_db}",
        "user": args.postgres_user,
        "password": args.postgres_password,
        "driver": "org.postgresql.Driver",
    }

    # Kh·ªüi t·∫°o Spark session v·ªõi PostgreSQL driver
    spark = create_spark_session(
        "Load Data to Serving Layer",
        config={
            "spark.jars": "/opt/bitnami/spark/jars/postgresql-42.7.0.jar",  # PostgreSQL JDBC driver
            "spark.sql.adaptive.enabled": "true",
            "spark.sql.adaptive.coalescePartitions.enabled": "true",
        },
    )

    try:
        # Load d·ªØ li·ªáu v√†o serving layer
        load_to_serving_layer(spark, args.date, args.property_type, postgres_config)
        print("‚úÖ Serving layer load completed successfully!")

    finally:
        spark.stop()
