"""
H·ª£p nh·∫•t d·ªØ li·ªáu t·ª´ nhi·ªÅu ngu·ªìn
"""

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col,
    current_timestamp,
    lit,
    regexp_replace,
    concat,
    md5,
    trim,
    monotonically_increasing_id,
    when,
    lower,
    element_at,
    split,
    size,
    count,
    sum as spark_sum,
    avg,
    stddev,
    min as spark_min,
    max as spark_max,
    percentile_approx,
)
import sys
import os
from datetime import datetime
import argparse
import uuid

# Th√™m th∆∞ m·ª•c g·ªëc v√†o sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.append(project_root)

from common.schema.common_schema import get_unified_property_schema
from common.utils.date_utils import (
    get_date_format,
    get_hdfs_path,
    generate_processing_id,
)
from common.utils.hdfs_utils import (
    check_hdfs_path_exists,
    ensure_hdfs_path,
    list_hdfs_files,
)
from common.utils.logging_utils import SparkJobLogger
from common.utils.data_quality_scoring import (
    calculate_data_quality_score,
    add_data_quality_issues_flag,
    add_quality_level,
)
from common.config.spark_config import create_spark_session


def unify_property_data(
    spark: SparkSession, input_date=None, property_type="house"
) -> DataFrame:
    """
    H·ª£p nh·∫•t d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n t·ª´ nhi·ªÅu ngu·ªìn

    Args:
        spark: SparkSession
        input_date: Ng√†y x·ª≠ l√Ω, ƒë·ªãnh d·∫°ng "YYYY-MM-DD". M·∫∑c ƒë·ªãnh l√† None (ng√†y h√¥m nay).
        property_type: Lo·∫°i b·∫•t ƒë·ªông s·∫£n ("house" ho·∫∑c "other"). M·∫∑c ƒë·ªãnh l√† "house".

    Returns:
        DataFrame ƒë√£ ƒë∆∞·ª£c h·ª£p nh·∫•t
    """
    logger = SparkJobLogger("unify_property_data")
    logger.start_job({"input_date": input_date, "property_type": property_type})

    # X√°c ƒë·ªãnh ng√†y x·ª≠ l√Ω
    if input_date is None:
        input_date = get_date_format()

    # T·∫°o ID x·ª≠ l√Ω
    processing_id = generate_processing_id("property_unify")

    # ƒê∆∞·ªùng d·∫´n ngu·ªìn v√† ƒë√≠ch
    silver_path = get_hdfs_path(
        "/data/realestate/processed/silver", "all", property_type, input_date
    )
    gold_path = get_hdfs_path(
        "/data/realestate/processed/gold", "unified", property_type, input_date
    )

    # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√≠ch t·ªìn t·∫°i
    ensure_hdfs_path(spark, gold_path)

    # L·∫•y schema th·ªëng nh·∫•t
    unified_schema = get_unified_property_schema()

    try:
        # ƒê∆∞·ªùng d·∫´n c√°c file silver t·ª´ t·ª´ng data source
        batdongsan_silver_path = get_hdfs_path(
            "/data/realestate/processed/silver", "batdongsan", property_type, input_date
        )
        chotot_silver_path = get_hdfs_path(
            "/data/realestate/processed/silver", "chotot", property_type, input_date
        )

        batdongsan_file = (
            f"{batdongsan_silver_path}/batdongsan_{input_date.replace('-', '')}.parquet"
        )
        chotot_file = (
            f"{chotot_silver_path}/chotot_{input_date.replace('-', '')}.parquet"
        )

        data_sources = []

        # ƒê·ªçc d·ªØ li·ªáu Batdongsan n·∫øu c√≥
        if check_hdfs_path_exists(spark, batdongsan_file):
            batdongsan_df = spark.read.parquet(batdongsan_file)
            logger.log_dataframe_info(batdongsan_df, "batdongsan_transformed")
            data_sources.append(batdongsan_df)
        else:
            logger.logger.warning(
                f"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu Batdongsan: {batdongsan_file}"
            )

        # ƒê·ªçc d·ªØ li·ªáu Chotot n·∫øu c√≥
        if check_hdfs_path_exists(spark, chotot_file):
            chotot_df = spark.read.parquet(chotot_file)
            logger.log_dataframe_info(chotot_df, "chotot_transformed")
            data_sources.append(chotot_df)
        else:
            logger.logger.warning(f"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu Chotot: {chotot_file}")

        # Ki·ªÉm tra xem c√≥ d·ªØ li·ªáu kh√¥ng
        if not data_sources:
            error_message = (
                f"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n√†o ƒë·ªÉ h·ª£p nh·∫•t cho ng√†y {input_date}"
            )
            logger.log_error(error_message)
            raise FileNotFoundError(error_message)

        # L·∫•y danh s√°ch c√°c tr∆∞·ªùng trong schema th·ªëng nh·∫•t
        schema_fields = [field.name for field in unified_schema.fields]

        # T·∫°o ID duy nh·∫•t v√† chu·∫©n h√≥a schema cho m·ªói DataFrame tr∆∞·ªõc khi h·ª£p nh·∫•t
        normalized_dfs = []
        for i, df in enumerate(data_sources):
            # Determine source name from actual data (now using 'source' field)
            source_name = "unknown"
            if "source" in df.columns:
                source_sample = df.select("source").limit(1).collect()
                if source_sample and source_sample[0]["source"]:
                    source_name = source_sample[0]["source"]
            elif "data_source" in df.columns:
                # Fallback to old field name if still exists
                source_sample = df.select("data_source").limit(1).collect()
                if source_sample and source_sample[0]["data_source"]:
                    source_name = source_sample[0]["data_source"]
                # Rename the field for consistency
                df = df.withColumnRenamed("data_source", "source")

            logger.logger.info(f"Processing source: {source_name}")

            # T·∫°o ID duy nh·∫•t d·ª±a tr√™n url v√† source (handle missing fields gracefully)
            if "url" in df.columns and "source" in df.columns:
                df = df.withColumn("id", md5(concat(col("url"), col("source"))))
            elif "url" in df.columns:
                df = df.withColumn("id", md5(col("url")))
            else:
                # Fallback to generated ID if no URL
                df = df.withColumn(
                    "id",
                    md5(
                        concat(
                            lit(source_name),
                            monotonically_increasing_id().cast("string"),
                        )
                    ),
                )

            # Log th√¥ng tin v·ªÅ schema c·ªßa ngu·ªìn d·ªØ li·ªáu
            logger.logger.info(f"Chu·∫©n h√≥a schema cho ngu·ªìn: {source_name}")
            logger.logger.info(f"C√°c c·ªôt hi·ªán c√≥: {', '.join(df.columns)}")
            logger.logger.info(f"S·ªë l∆∞·ª£ng c·ªôt hi·ªán c√≥: {len(df.columns)}")

            # 1. Th√™m c√°c c·ªôt c√≤n thi·∫øu v·ªõi gi√° tr·ªã null
            current_fields = df.columns
            missing_columns = [
                field for field in schema_fields if field not in current_fields
            ]
            if missing_columns:
                logger.logger.info(f"C√°c c·ªôt c·∫ßn th√™m: {', '.join(missing_columns)}")

            for field_name in schema_fields:
                if field_name not in current_fields:
                    matching_field = next(
                        (
                            field
                            for field in unified_schema.fields
                            if field.name == field_name
                        ),
                        None,
                    )
                    if matching_field:
                        df = df.withColumn(
                            field_name, lit(None).cast(matching_field.dataType)
                        )

            # 2. Ch·ªçn c√°c c·ªôt theo ƒë√∫ng th·ª© t·ª± trong schema th·ªëng nh·∫•t
            # Ch·ªâ ch·ªçn c√°c c·ªôt c√≥ trong schema
            select_fields = [field for field in schema_fields if field in df.columns]
            df = df.select(*select_fields)

            # 3. Th√™m c√°c c·ªôt c√≤n thi·∫øu sau khi select (ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª± ƒë√∫ng)
            for field_name in schema_fields:
                if field_name not in df.columns:
                    matching_field = next(
                        (
                            field
                            for field in unified_schema.fields
                            if field.name == field_name
                        ),
                        None,
                    )
                    if matching_field:
                        df = df.withColumn(
                            field_name, lit(None).cast(matching_field.dataType)
                        )

            # 4. S·∫Øp x·∫øp l·∫°i c√°c c·ªôt theo ƒë√∫ng th·ª© t·ª± c·ªßa schema th·ªëng nh·∫•t
            df = df.select(schema_fields)

            # Log th√¥ng tin sau khi chu·∫©n h√≥a
            logger.logger.info(
                f"Chu·∫©n h√≥a ho√†n t·∫•t. S·ªë l∆∞·ª£ng c·ªôt sau chu·∫©n h√≥a: {len(df.columns)}"
            )

            normalized_dfs.append(df)

        # H·ª£p nh·∫•t t·∫•t c·∫£ d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a
        if len(normalized_dfs) > 1:
            # N·∫øu c√≥ nhi·ªÅu ngu·ªìn, s·ª≠ d·ª•ng UNION
            unified_df = normalized_dfs[0]
            for df in normalized_dfs[1:]:
                unified_df = unified_df.union(df)
        else:
            # N·∫øu ch·ªâ c√≥ m·ªôt ngu·ªìn
            unified_df = normalized_dfs[0]

        # T·∫°o property_type d·ª±a v√†o data_type (c·ªôt ch√≠nh)
        # data_type l√† c·ªôt ch√≠nh cho property_type, house_type ch·ªâ l√† metadata c·ªßa Chotot
        unified_df = unified_df.withColumn(
            "property_type",
            when(
                col("data_type").isNotNull() & (col("data_type") != ""),
                col("data_type"),
            )
            .when(col("bedroom").isNotNull() & (col("bedroom") > 0), lit("HOUSE"))
            .when(col("area").isNotNull() & (col("area") > 0), lit("LAND"))
            .otherwise(lit("UNKNOWN")),
        )

        # ‚úÖ ADDRESS PARSING ƒê√É ƒê∆Ø·ª¢C TH·ª∞C HI·ªÜN ·ªû TRANSFORM STEP
        # Kh√¥ng c·∫ßn x·ª≠ l√Ω l·∫°i ·ªü ƒë√¢y, c√°c tr∆∞·ªùng street, ward, district, province ƒë√£ c√≥ s·∫µn

        # Th√™m th√¥ng tin x·ª≠ l√Ω unify (ch·ªâ processing_id, timestamp ƒë√£ c√≥ t·ª´ transform jobs)
        unified_df = unified_df.withColumn("processing_id", lit(processing_id))

        # === T√çNH ƒêI·ªÇM CH·∫§T L∆Ø·ª¢NG TH·ªêNG NH·∫§T ===
        logger.logger.info("T√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu th·ªëng nh·∫•t...")

        # T√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng s·ª≠ d·ª•ng module chung
        unified_df = calculate_data_quality_score(unified_df)

        # Th√™m flags v·ªÅ v·∫•n ƒë·ªÅ ch·∫•t l∆∞·ª£ng
        unified_df = add_data_quality_issues_flag(unified_df)

        # Th√™m level ch·∫•t l∆∞·ª£ng
        unified_df = add_quality_level(unified_df)

        # Clean up helper columns
        unified_df = unified_df.drop(
            "has_valid_price",
            "has_valid_area",
            "has_valid_location",
            "has_coordinates",
            "has_valid_bedroom",
            "has_valid_bathroom",
        )

        logger.logger.info("Ho√†n th√†nh t√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng th·ªëng nh·∫•t!")

        # === TH·ªêNG K√ä TO√ÄN DI·ªÜN SAU KHI UNIFY ===
        logger.logger.info("üéØ T√≠nh th·ªëng k√™ to√†n di·ªán cho d·ªØ li·ªáu ƒë√£ unify...")

        # Cache ƒë·ªÉ t·ªëi ∆∞u performance
        unified_df.cache()

        total_records = unified_df.count()
        logger.logger.info(f"üìä T·ªîNG K·∫æT UNIFIED DATASET:")
        logger.logger.info(f"   T·ªïng s·ªë records: {total_records:,}")

        # Th·ªëng k√™ theo ngu·ªìn d·ªØ li·ªáu
        source_stats = unified_df.groupBy("source").count().collect()
        logger.logger.info(f"üìã TH·ªêNG K√ä THEO NGU·ªíN D·ªÆ LI·ªÜU:")
        for row in source_stats:
            logger.logger.info(f"   {row['source']}: {row['count']:,} records")

        # Th·ªëng k√™ theo lo·∫°i property
        property_stats = unified_df.groupBy("property_type").count().collect()
        logger.logger.info(f"üè† TH·ªêNG K√ä THEO LO·∫†I B·∫§T ƒê·ªòNG S·∫¢N:")
        for row in property_stats:
            logger.logger.info(f"   {row['property_type']}: {row['count']:,} records")

        # Th·ªëng k√™ theo quality level
        quality_stats = unified_df.groupBy("quality_level").count().collect()
        logger.logger.info(f"‚≠ê TH·ªêNG K√ä CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU:")
        for row in quality_stats:
            logger.logger.info(f"   {row['quality_level']}: {row['count']:,} records")

        # Th·ªëng k√™ missing data
        missing_stats = unified_df.select(
            count("*").alias("total"),
            spark_sum(when(col("price").isNull(), 1).otherwise(0)).alias(
                "missing_price"
            ),
            spark_sum(when(col("area").isNull(), 1).otherwise(0)).alias("missing_area"),
            spark_sum(when(col("bedroom").isNull(), 1).otherwise(0)).alias(
                "missing_bedroom"
            ),
            spark_sum(when(col("bathroom").isNull(), 1).otherwise(0)).alias(
                "missing_bathroom"
            ),
            spark_sum(
                when(col("latitude").isNull() | col("longitude").isNull(), 1).otherwise(
                    0
                )
            ).alias("missing_coordinates"),
            spark_sum(
                when(col("location").isNull() | (col("location") == ""), 1).otherwise(0)
            ).alias("missing_location"),
        ).collect()[0]

        logger.logger.info(f"‚ùå TH·ªêNG K√ä D·ªÆ LI·ªÜU THI·∫æU:")
        logger.logger.info(
            f"   Missing price: {missing_stats['missing_price']:,} ({missing_stats['missing_price']/total_records*100:.1f}%)"
        )
        logger.logger.info(
            f"   Missing area: {missing_stats['missing_area']:,} ({missing_stats['missing_area']/total_records*100:.1f}%)"
        )
        logger.logger.info(
            f"   Missing bedroom: {missing_stats['missing_bedroom']:,} ({missing_stats['missing_bedroom']/total_records*100:.1f}%)"
        )
        logger.logger.info(
            f"   Missing bathroom: {missing_stats['missing_bathroom']:,} ({missing_stats['missing_bathroom']/total_records*100:.1f}%)"
        )
        logger.logger.info(
            f"   Missing coordinates: {missing_stats['missing_coordinates']:,} ({missing_stats['missing_coordinates']/total_records*100:.1f}%)"
        )
        logger.logger.info(
            f"   Missing location: {missing_stats['missing_location']:,} ({missing_stats['missing_location']/total_records*100:.1f}%)"
        )

        # Th·ªëng k√™ gi√° tr·ªã
        # L·ªçc d·ªØ li·ªáu c√≥ gi√° h·ª£p l·ªá ƒë·ªÉ t√≠nh stats
        valid_price_df = unified_df.filter(
            col("price").isNotNull() & (col("price") > 0)
        )
        valid_area_df = unified_df.filter(col("area").isNotNull() & (col("area") > 0))

        if valid_price_df.count() > 0:
            price_stats = valid_price_df.select(
                avg("price").alias("avg_price"),
                stddev("price").alias("stddev_price"),
                spark_min("price").alias("min_price"),
                spark_max("price").alias("max_price"),
                percentile_approx("price", 0.5).alias("median_price"),
            ).collect()[0]

            logger.logger.info(
                f"üí∞ TH·ªêNG K√ä GI√Å ({valid_price_df.count():,} records c√≥ gi√° h·ª£p l·ªá):"
            )
            logger.logger.info(
                f"   Gi√° trung b√¨nh: {price_stats['avg_price']/1_000_000_000:.2f} t·ª∑ VND"
            )
            logger.logger.info(
                f"   Gi√° median: {price_stats['median_price']/1_000_000_000:.2f} t·ª∑ VND"
            )
            logger.logger.info(
                f"   Gi√° min: {price_stats['min_price']/1_000_000:.0f} tri·ªáu VND"
            )
            logger.logger.info(
                f"   Gi√° max: {price_stats['max_price']/1_000_000_000:.1f} t·ª∑ VND"
            )
            logger.logger.info(
                f"   ƒê·ªô l·ªách chu·∫©n: {price_stats['stddev_price']/1_000_000_000:.2f} t·ª∑ VND"
            )

        if valid_area_df.count() > 0:
            area_stats = valid_area_df.select(
                avg("area").alias("avg_area"),
                stddev("area").alias("stddev_area"),
                spark_min("area").alias("min_area"),
                spark_max("area").alias("max_area"),
                percentile_approx("area", 0.5).alias("median_area"),
            ).collect()[0]

            logger.logger.info(
                f"üìê TH·ªêNG K√ä DI·ªÜN T√çCH ({valid_area_df.count():,} records c√≥ di·ªán t√≠ch h·ª£p l·ªá):"
            )
            logger.logger.info(
                f"   Di·ªán t√≠ch trung b√¨nh: {area_stats['avg_area']:.1f} m¬≤"
            )
            logger.logger.info(
                f"   Di·ªán t√≠ch median: {area_stats['median_area']:.1f} m¬≤"
            )
            logger.logger.info(f"   Di·ªán t√≠ch min: {area_stats['min_area']:.0f} m¬≤")
            logger.logger.info(f"   Di·ªán t√≠ch max: {area_stats['max_area']:.0f} m¬≤")
            logger.logger.info(f"   ƒê·ªô l·ªách chu·∫©n: {area_stats['stddev_area']:.1f} m¬≤")

        # Th·ªëng k√™ data quality score
        score_stats = unified_df.select(
            avg("data_quality_score").alias("avg_score"),
            stddev("data_quality_score").alias("stddev_score"),
            spark_min("data_quality_score").alias("min_score"),
            spark_max("data_quality_score").alias("max_score"),
            percentile_approx("data_quality_score", 0.5).alias("median_score"),
        ).collect()[0]

        logger.logger.info(f"üéØ TH·ªêNG K√ä ƒêI·ªÇM CH·∫§T L∆Ø·ª¢NG (scale 0-100):")
        logger.logger.info(f"   ƒêi·ªÉm trung b√¨nh: {score_stats['avg_score']:.1f}")
        logger.logger.info(f"   ƒêi·ªÉm median: {score_stats['median_score']:.1f}")
        logger.logger.info(f"   ƒêi·ªÉm min: {score_stats['min_score']:.0f}")
        logger.logger.info(f"   ƒêi·ªÉm max: {score_stats['max_score']:.0f}")
        logger.logger.info(f"   ƒê·ªô l·ªách chu·∫©n: {score_stats['stddev_score']:.1f}")

        # Th·ªëng k√™ theo t·ªânh/th√†nh ph·ªë (top 10)
        if "province" in unified_df.columns:
            province_stats = (
                unified_df.filter(col("province").isNotNull() & (col("province") != ""))
                .groupBy("province")
                .count()
                .orderBy(col("count").desc())
                .limit(10)
                .collect()
            )

            logger.logger.info(f"üåç TOP 10 T·ªàNH/TH√ÄNH PH·ªê:")
            for row in province_stats:
                logger.logger.info(f"   {row['province']}: {row['count']:,} records")

        logger.logger.info("‚úÖ Ho√†n th√†nh th·ªëng k√™ to√†n di·ªán!")

        # Kh√¥ng c·∫ßn √°p d·ª•ng l·∫°i schema v√¨ ƒë√£ chu·∫©n h√≥a DataFrames tr∆∞·ªõc khi h·ª£p nh·∫•t
        # C√°c c·ªôt ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a v√† s·∫Øp x·∫øp theo th·ª© t·ª± c·ªßa schema th·ªëng nh·∫•t

        # Log th√¥ng tin sau khi h·ª£p nh·∫•t
        logger.log_dataframe_info(unified_df, "unified_data")

        # Ghi d·ªØ li·ªáu ra
        output_path = os.path.join(
            gold_path, f"unified_{property_type}_{input_date.replace('-', '')}.parquet"
        )
        unified_df.write.mode("overwrite").parquet(output_path)

        logger.logger.info(f"ƒê√£ ghi {unified_df.count()} b·∫£n ghi v√†o {output_path}")
        logger.end_job()

        return unified_df

    except Exception as e:
        logger.log_error("L·ªói khi h·ª£p nh·∫•t d·ªØ li·ªáu", e)
        raise


def parse_args():
    """
    Parse command line arguments
    """
    parser = argparse.ArgumentParser(description="Unify Property Data")
    parser.add_argument("--date", type=str, help="Processing date in YYYY-MM-DD format")
    parser.add_argument(
        "--property-type",
        type=str,
        default="house",
        choices=["house", "other"],
        help="Property type",
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    # Kh·ªüi t·∫°o Spark session
    spark = create_spark_session("Unify Property Data")

    try:
        # H·ª£p nh·∫•t d·ªØ li·ªáu
        unify_property_data(spark, args.date, args.property_type)

    finally:
        spark.stop()
