"""
Há»£p nháº¥t dá»¯ liá»‡u tá»« nhiá»u nguá»“n
"""

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col,
    current_timestamp,
    lit,
    regexp_replace,
    concat,
    md5,
    trim,
    monotonically_increasing_id,
    when,
    lower,
    element_at,
    split,
    size,
    count,
    sum as spark_sum,
    avg,
    stddev,
    min as spark_min,
    max as spark_max,
    percentile_approx,
)
import sys
import os
from datetime import datetime
import argparse
import uuid

# ThÃªm thÆ° má»¥c gá»‘c vÃ o sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.append(project_root)

from common.schema.common_schema import get_unified_property_schema
from common.utils.date_utils import (
    get_date_format,
    get_hdfs_path,
    generate_processing_id,
)
from common.utils.hdfs_utils import (
    check_hdfs_path_exists,
    ensure_hdfs_path,
    list_hdfs_files,
)
from common.utils.logging_utils import SparkJobLogger
from common.utils.data_quality_scoring import (
    calculate_data_quality_score,
    add_data_quality_issues_flag,
    add_quality_level,
)
from common.config.spark_config import create_spark_session
from common.utils.duplicate_detection import apply_unify_deduplication


def unify_property_data(
    spark: SparkSession, input_date=None, property_type="house"
) -> DataFrame:
    """
    Há»£p nháº¥t dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n tá»« nhiá»u nguá»“n

    Args:
        spark: SparkSession
        input_date: NgÃ y xá»­ lÃ½, Ä‘á»‹nh dáº¡ng "YYYY-MM-DD". Máº·c Ä‘á»‹nh lÃ  None (ngÃ y hÃ´m nay).
        property_type: Loáº¡i báº¥t Ä‘á»™ng sáº£n ("house" hoáº·c "other"). Máº·c Ä‘á»‹nh lÃ  "house".

    Returns:
        DataFrame Ä‘Ã£ Ä‘Æ°á»£c há»£p nháº¥t
    """
    logger = SparkJobLogger("unify_property_data")
    logger.start_job({"input_date": input_date, "property_type": property_type})

    # XÃ¡c Ä‘á»‹nh ngÃ y xá»­ lÃ½
    if input_date is None:
        input_date = get_date_format()

    # Táº¡o ID xá»­ lÃ½
    processing_id = generate_processing_id("property_unify")

    # ÄÆ°á»ng dáº«n nguá»“n vÃ  Ä‘Ã­ch
    silver_path = get_hdfs_path(
        "/data/realestate/processed/silver", "all", property_type, input_date
    )
    gold_path = get_hdfs_path(
        "/data/realestate/processed/gold", "unified", property_type, input_date
    )

    # Äáº£m báº£o Ä‘Æ°á»ng dáº«n Ä‘Ã­ch tá»“n táº¡i
    ensure_hdfs_path(spark, gold_path)

    # Láº¥y schema thá»‘ng nháº¥t
    unified_schema = get_unified_property_schema()

    try:
        # ÄÆ°á»ng dáº«n cÃ¡c file silver tá»« tá»«ng data source
        batdongsan_silver_path = get_hdfs_path(
            "/data/realestate/processed/silver", "batdongsan", property_type, input_date
        )
        chotot_silver_path = get_hdfs_path(
            "/data/realestate/processed/silver", "chotot", property_type, input_date
        )

        batdongsan_file = (
            f"{batdongsan_silver_path}/batdongsan_{input_date.replace('-', '')}.parquet"
        )
        chotot_file = (
            f"{chotot_silver_path}/chotot_{input_date.replace('-', '')}.parquet"
        )

        data_sources = []

        # Äá»c dá»¯ liá»‡u Batdongsan náº¿u cÃ³
        if check_hdfs_path_exists(spark, batdongsan_file):
            batdongsan_df = spark.read.parquet(batdongsan_file)
            logger.log_dataframe_info(batdongsan_df, "batdongsan_transformed")
            data_sources.append(batdongsan_df)
        else:
            logger.logger.warning(
                f"KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u Batdongsan: {batdongsan_file}"
            )

        # Äá»c dá»¯ liá»‡u Chotot náº¿u cÃ³
        if check_hdfs_path_exists(spark, chotot_file):
            chotot_df = spark.read.parquet(chotot_file)
            logger.log_dataframe_info(chotot_df, "chotot_transformed")
            data_sources.append(chotot_df)
        else:
            logger.logger.warning(f"KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u Chotot: {chotot_file}")

        # Kiá»ƒm tra xem cÃ³ dá»¯ liá»‡u khÃ´ng
        if not data_sources:
            error_message = (
                f"KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u nÃ o Ä‘á»ƒ há»£p nháº¥t cho ngÃ y {input_date}"
            )
            logger.log_error(error_message)
            raise FileNotFoundError(error_message)

        # Láº¥y danh sÃ¡ch cÃ¡c trÆ°á»ng trong schema thá»‘ng nháº¥t
        schema_fields = [field.name for field in unified_schema.fields]

        # Táº¡o ID duy nháº¥t vÃ  chuáº©n hÃ³a schema cho má»—i DataFrame trÆ°á»›c khi há»£p nháº¥t
        normalized_dfs = []
        for i, df in enumerate(data_sources):
            # Determine source name from actual data (now using 'source' field)
            source_name = "unknown"
            if "source" in df.columns:
                source_sample = df.select("source").limit(1).collect()
                if source_sample and source_sample[0]["source"]:
                    source_name = source_sample[0]["source"]
            elif "data_source" in df.columns:
                # Fallback to old field name if still exists
                source_sample = df.select("data_source").limit(1).collect()
                if source_sample and source_sample[0]["data_source"]:
                    source_name = source_sample[0]["data_source"]
                # Rename the field for consistency
                df = df.withColumnRenamed("data_source", "source")

            logger.logger.info(f"Processing source: {source_name}")

            # Táº¡o ID duy nháº¥t dá»±a trÃªn url vÃ  source (handle missing fields gracefully)
            if "url" in df.columns and "source" in df.columns:
                df = df.withColumn("id", md5(concat(col("url"), col("source"))))
            elif "url" in df.columns:
                df = df.withColumn("id", md5(col("url")))
            else:
                # Fallback to generated ID if no URL
                df = df.withColumn(
                    "id",
                    md5(
                        concat(
                            lit(source_name),
                            monotonically_increasing_id().cast("string"),
                        )
                    ),
                )

            # Log thÃ´ng tin vá» schema cá»§a nguá»“n dá»¯ liá»‡u
            logger.logger.info(f"Chuáº©n hÃ³a schema cho nguá»“n: {source_name}")
            logger.logger.info(f"CÃ¡c cá»™t hiá»‡n cÃ³: {', '.join(df.columns)}")
            logger.logger.info(f"Sá»‘ lÆ°á»£ng cá»™t hiá»‡n cÃ³: {len(df.columns)}")

            # 1. ThÃªm cÃ¡c cá»™t cÃ²n thiáº¿u vá»›i giÃ¡ trá»‹ null
            current_fields = df.columns
            missing_columns = [
                field for field in schema_fields if field not in current_fields
            ]
            if missing_columns:
                logger.logger.info(f"CÃ¡c cá»™t cáº§n thÃªm: {', '.join(missing_columns)}")

            for field_name in schema_fields:
                if field_name not in current_fields:
                    matching_field = next(
                        (
                            field
                            for field in unified_schema.fields
                            if field.name == field_name
                        ),
                        None,
                    )
                    if matching_field:
                        df = df.withColumn(
                            field_name, lit(None).cast(matching_field.dataType)
                        )

            # 2. Chá»n cÃ¡c cá»™t theo Ä‘Ãºng thá»© tá»± trong schema thá»‘ng nháº¥t
            # Chá»‰ chá»n cÃ¡c cá»™t cÃ³ trong schema
            select_fields = [field for field in schema_fields if field in df.columns]
            df = df.select(*select_fields)

            # 3. ThÃªm cÃ¡c cá»™t cÃ²n thiáº¿u sau khi select (Ä‘á»ƒ Ä‘áº£m báº£o thá»© tá»± Ä‘Ãºng)
            for field_name in schema_fields:
                if field_name not in df.columns:
                    matching_field = next(
                        (
                            field
                            for field in unified_schema.fields
                            if field.name == field_name
                        ),
                        None,
                    )
                    if matching_field:
                        df = df.withColumn(
                            field_name, lit(None).cast(matching_field.dataType)
                        )

            # 4. Sáº¯p xáº¿p láº¡i cÃ¡c cá»™t theo Ä‘Ãºng thá»© tá»± cá»§a schema thá»‘ng nháº¥t
            df = df.select(schema_fields)

            # Log thÃ´ng tin sau khi chuáº©n hÃ³a
            logger.logger.info(
                f"Chuáº©n hÃ³a hoÃ n táº¥t. Sá»‘ lÆ°á»£ng cá»™t sau chuáº©n hÃ³a: {len(df.columns)}"
            )

            normalized_dfs.append(df)

        # Há»£p nháº¥t táº¥t cáº£ dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a
        if len(normalized_dfs) > 1:
            # Náº¿u cÃ³ nhiá»u nguá»“n, sá»­ dá»¥ng UNION
            unified_df = normalized_dfs[0]
            for df in normalized_dfs[1:]:
                unified_df = unified_df.union(df)
        else:
            # Náº¿u chá»‰ cÃ³ má»™t nguá»“n
            unified_df = normalized_dfs[0]

        # Táº¡o property_type dá»±a vÃ o data_type (cá»™t chÃ­nh)
        # data_type lÃ  cá»™t chÃ­nh cho property_type, house_type chá»‰ lÃ  metadata cá»§a Chotot
        unified_df = unified_df.withColumn(
            "property_type",
            when(
                col("data_type").isNotNull() & (col("data_type") != ""),
                col("data_type"),
            )
            .when(col("bedroom").isNotNull() & (col("bedroom") > 0), lit("HOUSE"))
            .when(col("area").isNotNull() & (col("area") > 0), lit("LAND"))
            .otherwise(lit("UNKNOWN")),
        )

        # âœ… ADDRESS PARSING ÄÃƒ ÄÆ¯á»¢C THá»°C HIá»†N á» TRANSFORM STEP
        # KhÃ´ng cáº§n xá»­ lÃ½ láº¡i á»Ÿ Ä‘Ã¢y, cÃ¡c trÆ°á»ng street, ward, district, province Ä‘Ã£ cÃ³ sáºµn

        # ThÃªm thÃ´ng tin xá»­ lÃ½ unify (chá»‰ processing_id, timestamp Ä‘Ã£ cÃ³ tá»« transform jobs)
        unified_df = unified_df.withColumn("processing_id", lit(processing_id))

        # === TÃNH ÄIá»‚M CHáº¤T LÆ¯á»¢NG THá»NG NHáº¤T ===
        logger.logger.info("TÃ­nh Ä‘iá»ƒm cháº¥t lÆ°á»£ng dá»¯ liá»‡u thá»‘ng nháº¥t...")

        # TÃ­nh Ä‘iá»ƒm cháº¥t lÆ°á»£ng sá»­ dá»¥ng module chung
        unified_df = calculate_data_quality_score(unified_df)

        # ThÃªm flags vá» váº¥n Ä‘á» cháº¥t lÆ°á»£ng
        unified_df = add_data_quality_issues_flag(unified_df)

        # ThÃªm level cháº¥t lÆ°á»£ng
        unified_df = add_quality_level(unified_df)

        # Clean up helper columns
        unified_df = unified_df.drop(
            "has_valid_price",
            "has_valid_area",
            "has_valid_location",
            "has_coordinates",
            "has_valid_bedroom",
            "has_valid_bathroom",
        )

        logger.logger.info("HoÃ n thÃ nh tÃ­nh Ä‘iá»ƒm cháº¥t lÆ°á»£ng thá»‘ng nháº¥t!")

        # === Duplication Detection ===
        logger.logger.info("ğŸ” Báº¯t Ä‘áº§u phÃ¡t hiá»‡n vÃ  loáº¡i bá» trÃ¹ng láº·p...")

        # Log count trÆ°á»›c khi deduplication
        pre_dedup_count = unified_df.count()
        logger.logger.info(f"ğŸ“Š Records trÆ°á»›c deduplication: {pre_dedup_count:,}")

        # Apply comprehensive deduplication
        unified_df = apply_unify_deduplication(unified_df)

        # Log káº¿t quáº£ deduplication
        post_dedup_count = unified_df.count()
        duplicates_removed = pre_dedup_count - post_dedup_count
        logger.logger.info(f"âœ… Deduplication hoÃ n thÃ nh:")
        logger.logger.info(f"   ğŸ“¤ Records sau deduplication: {post_dedup_count:,}")
        logger.logger.info(f"   ğŸ—‘ï¸ Duplicates loáº¡i bá»: {duplicates_removed:,}")
        logger.logger.info(
            f"   ğŸ“‰ Reduction rate: {duplicates_removed/pre_dedup_count*100:.1f}%"
        )

        # === THá»NG KÃŠ TOÃ€N DIá»†N SAU KHI UNIFY ===
        logger.logger.info("ğŸ¯ TÃ­nh thá»‘ng kÃª toÃ n diá»‡n cho dá»¯ liá»‡u Ä‘Ã£ unify...")

        # Cache Ä‘á»ƒ tá»‘i Æ°u performance
        unified_df.cache()

        total_records = unified_df.count()
        logger.logger.info(f"ğŸ“Š Tá»”NG Káº¾T UNIFIED DATASET:")
        logger.logger.info(f"   Tá»•ng sá»‘ records: {total_records:,}")

        # Thá»‘ng kÃª theo nguá»“n dá»¯ liá»‡u
        source_stats = unified_df.groupBy("source").count().collect()
        logger.logger.info(f"ğŸ“‹ THá»NG KÃŠ THEO NGUá»’N Dá»® LIá»†U:")
        for row in source_stats:
            logger.logger.info(f"   {row['source']}: {row['count']:,} records")

        # Thá»‘ng kÃª theo loáº¡i property
        property_stats = unified_df.groupBy("property_type").count().collect()
        logger.logger.info(f"ğŸ  THá»NG KÃŠ THEO LOáº I Báº¤T Äá»˜NG Sáº¢N:")
        for row in property_stats:
            logger.logger.info(f"   {row['property_type']}: {row['count']:,} records")

        # Thá»‘ng kÃª theo quality level
        quality_stats = unified_df.groupBy("quality_level").count().collect()
        logger.logger.info(f"â­ THá»NG KÃŠ CHáº¤T LÆ¯á»¢NG Dá»® LIá»†U:")
        for row in quality_stats:
            logger.logger.info(f"   {row['quality_level']}: {row['count']:,} records")

        # Thá»‘ng kÃª missing data
        missing_stats = unified_df.select(
            count("*").alias("total"),
            spark_sum(when(col("price").isNull(), 1).otherwise(0)).alias(
                "missing_price"
            ),
            spark_sum(when(col("area").isNull(), 1).otherwise(0)).alias("missing_area"),
            spark_sum(when(col("bedroom").isNull(), 1).otherwise(0)).alias(
                "missing_bedroom"
            ),
            spark_sum(when(col("bathroom").isNull(), 1).otherwise(0)).alias(
                "missing_bathroom"
            ),
            spark_sum(
                when(col("latitude").isNull() | col("longitude").isNull(), 1).otherwise(
                    0
                )
            ).alias("missing_coordinates"),
            spark_sum(
                when(col("location").isNull() | (col("location") == ""), 1).otherwise(0)
            ).alias("missing_location"),
        ).collect()[0]

        logger.logger.info(f"âŒ THá»NG KÃŠ Dá»® LIá»†U THIáº¾U:")
        logger.logger.info(
            f"   Missing price: {missing_stats['missing_price']:,} ({missing_stats['missing_price']/total_records*100:.1f}%)"
        )
        logger.logger.info(
            f"   Missing area: {missing_stats['missing_area']:,} ({missing_stats['missing_area']/total_records*100:.1f}%)"
        )
        logger.logger.info(
            f"   Missing bedroom: {missing_stats['missing_bedroom']:,} ({missing_stats['missing_bedroom']/total_records*100:.1f}%)"
        )
        logger.logger.info(
            f"   Missing bathroom: {missing_stats['missing_bathroom']:,} ({missing_stats['missing_bathroom']/total_records*100:.1f}%)"
        )
        logger.logger.info(
            f"   Missing coordinates: {missing_stats['missing_coordinates']:,} ({missing_stats['missing_coordinates']/total_records*100:.1f}%)"
        )
        logger.logger.info(
            f"   Missing location: {missing_stats['missing_location']:,} ({missing_stats['missing_location']/total_records*100:.1f}%)"
        )

        # Thá»‘ng kÃª giÃ¡ trá»‹
        # Lá»c dá»¯ liá»‡u cÃ³ giÃ¡ há»£p lá»‡ Ä‘á»ƒ tÃ­nh stats
        valid_price_df = unified_df.filter(
            col("price").isNotNull() & (col("price") > 0)
        )
        valid_area_df = unified_df.filter(col("area").isNotNull() & (col("area") > 0))

        if valid_price_df.count() > 0:
            price_stats = valid_price_df.select(
                avg("price").alias("avg_price"),
                stddev("price").alias("stddev_price"),
                spark_min("price").alias("min_price"),
                spark_max("price").alias("max_price"),
                percentile_approx("price", 0.5).alias("median_price"),
            ).collect()[0]

            logger.logger.info(
                f"ğŸ’° THá»NG KÃŠ GIÃ ({valid_price_df.count():,} records cÃ³ giÃ¡ há»£p lá»‡):"
            )
            logger.logger.info(
                f"   GiÃ¡ trung bÃ¬nh: {price_stats['avg_price']/1_000_000_000:.2f} tá»· VND"
            )
            logger.logger.info(
                f"   GiÃ¡ median: {price_stats['median_price']/1_000_000_000:.2f} tá»· VND"
            )
            logger.logger.info(
                f"   GiÃ¡ min: {price_stats['min_price']/1_000_000:.0f} triá»‡u VND"
            )
            logger.logger.info(
                f"   GiÃ¡ max: {price_stats['max_price']/1_000_000_000:.1f} tá»· VND"
            )
            logger.logger.info(
                f"   Äá»™ lá»‡ch chuáº©n: {price_stats['stddev_price']/1_000_000_000:.2f} tá»· VND"
            )

        if valid_area_df.count() > 0:
            area_stats = valid_area_df.select(
                avg("area").alias("avg_area"),
                stddev("area").alias("stddev_area"),
                spark_min("area").alias("min_area"),
                spark_max("area").alias("max_area"),
                percentile_approx("area", 0.5).alias("median_area"),
            ).collect()[0]

            logger.logger.info(
                f"ğŸ“ THá»NG KÃŠ DIá»†N TÃCH ({valid_area_df.count():,} records cÃ³ diá»‡n tÃ­ch há»£p lá»‡):"
            )
            logger.logger.info(
                f"   Diá»‡n tÃ­ch trung bÃ¬nh: {area_stats['avg_area']:.1f} mÂ²"
            )
            logger.logger.info(
                f"   Diá»‡n tÃ­ch median: {area_stats['median_area']:.1f} mÂ²"
            )
            logger.logger.info(f"   Diá»‡n tÃ­ch min: {area_stats['min_area']:.0f} mÂ²")
            logger.logger.info(f"   Diá»‡n tÃ­ch max: {area_stats['max_area']:.0f} mÂ²")
            logger.logger.info(f"   Äá»™ lá»‡ch chuáº©n: {area_stats['stddev_area']:.1f} mÂ²")

        # Thá»‘ng kÃª data quality score
        score_stats = unified_df.select(
            avg("data_quality_score").alias("avg_score"),
            stddev("data_quality_score").alias("stddev_score"),
            spark_min("data_quality_score").alias("min_score"),
            spark_max("data_quality_score").alias("max_score"),
            percentile_approx("data_quality_score", 0.5).alias("median_score"),
        ).collect()[0]

        logger.logger.info(f"ğŸ¯ THá»NG KÃŠ ÄIá»‚M CHáº¤T LÆ¯á»¢NG (scale 0-100):")
        logger.logger.info(f"   Äiá»ƒm trung bÃ¬nh: {score_stats['avg_score']:.1f}")
        logger.logger.info(f"   Äiá»ƒm median: {score_stats['median_score']:.1f}")
        logger.logger.info(f"   Äiá»ƒm min: {score_stats['min_score']:.0f}")
        logger.logger.info(f"   Äiá»ƒm max: {score_stats['max_score']:.0f}")
        logger.logger.info(f"   Äá»™ lá»‡ch chuáº©n: {score_stats['stddev_score']:.1f}")

        # Thá»‘ng kÃª theo tá»‰nh/thÃ nh phá»‘ (top 10)
        if "province" in unified_df.columns:
            province_stats = (
                unified_df.filter(col("province").isNotNull() & (col("province") != ""))
                .groupBy("province")
                .count()
                .orderBy(col("count").desc())
                .limit(10)
                .collect()
            )

            logger.logger.info(f"ğŸŒ TOP 10 Tá»ˆNH/THÃ€NH PHá»:")
            for row in province_stats:
                logger.logger.info(f"   {row['province']}: {row['count']:,} records")

        logger.logger.info("âœ… HoÃ n thÃ nh thá»‘ng kÃª toÃ n diá»‡n!")

        # === FINAL SUMMARY ===
        final_count = unified_df.count()
        logger.logger.info(f"ğŸ UNIFY PIPELINE COMPLETED SUCCESSFULLY!")
        logger.logger.info(f"ğŸ“Š FINAL SUMMARY:")
        logger.logger.info(f"   ğŸ“¥ Raw input records: {pre_dedup_count:,}")
        logger.logger.info(f"   ğŸ”„ After deduplication: {final_count:,}")
        logger.logger.info(
            f"   ğŸ“‰ Total duplicates removed: {pre_dedup_count - final_count:,}"
        )
        logger.logger.info(
            f"   âš¡ Processing efficiency: {final_count/pre_dedup_count*100:.1f}% data retained"
        )

        # KhÃ´ng cáº§n Ã¡p dá»¥ng láº¡i schema vÃ¬ Ä‘Ã£ chuáº©n hÃ³a DataFrames trÆ°á»›c khi há»£p nháº¥t
        # CÃ¡c cá»™t Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a vÃ  sáº¯p xáº¿p theo thá»© tá»± cá»§a schema thá»‘ng nháº¥t

        # Log thÃ´ng tin sau khi há»£p nháº¥t
        logger.log_dataframe_info(unified_df, "unified_data_final")

        # Ghi dá»¯ liá»‡u ra
        output_path = os.path.join(
            gold_path, f"unified_{property_type}_{input_date.replace('-', '')}.parquet"
        )
        logger.logger.info(f"ğŸ’¾ Writing data to: {output_path}")
        unified_df.write.mode("overwrite").parquet(output_path)

        logger.logger.info(
            f"âœ… Successfully wrote {final_count:,} records to Gold layer"
        )
        logger.end_job()

        return unified_df

    except Exception as e:
        logger.log_error("Lá»—i khi há»£p nháº¥t dá»¯ liá»‡u", e)
        raise


def parse_args():
    """
    Parse command line arguments
    """
    parser = argparse.ArgumentParser(description="Unify Property Data")
    parser.add_argument("--date", type=str, help="Processing date in YYYY-MM-DD format")
    parser.add_argument(
        "--property-type",
        type=str,
        default="house",
        choices=["house", "other"],
        help="Property type",
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    # Khá»Ÿi táº¡o Spark session
    spark = create_spark_session("Unify Property Data")

    try:
        # Há»£p nháº¥t dá»¯ liá»‡u
        unify_property_data(spark, args.date, args.property_type)

    finally:
        spark.stop()
