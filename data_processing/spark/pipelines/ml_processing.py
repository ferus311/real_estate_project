"""
Pipeline xá»­ lÃ½ Machine Learning riÃªng biá»‡t
Gold â†’ Data Preparation â†’ Trained Models

TÃ¡ch biá»‡t hoÃ n toÃ n khá»i ETL pipeline Ä‘á»ƒ:
- Äá»™c láº­p vá» tÃ i nguyÃªn vÃ  cáº¥u hÃ¬nh
- Lá»‹ch cháº¡y khÃ¡c nhau (ETL hÃ ng ngÃ y, ML theo tuáº§n/thÃ¡ng)
- Dá»… báº£o trÃ¬ vÃ  phÃ¡t triá»ƒn
- TÃ¡ch biá»‡t team ownership

Pipeline cÃ³ 2 stages chÃ­nh:
1. Data Preparation: Gold â†’ Cleaned Data + Feature Engineering
2. Model Training: Prepared Data â†’ Trained Models
"""

from pyspark.sql import SparkSession
from datetime import datetime, timedelta
import sys
import os
import argparse

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(project_root)

from common.utils.date_utils import get_date_format
from common.utils.logging_utils import SparkJobLogger
from common.config.spark_config import create_optimized_ml_spark_session

# Import correct ML pipeline modules from ml/pipelines/
from ml.pipelines.data_preparation import MLDataPreprocessor
from ml.pipelines.model_training import MLTrainer


def run_data_preparation_stage(
    spark: SparkSession, input_date: str, property_type: str, logger
) -> bool:
    """Stage 1: Data Preparation (Gold â†’ Cleaned Data + Feature Engineering)"""
    logger.logger.info("ğŸ”§ Stage 1: Data Preparation (Cleaning + Feature Engineering)")

    try:
        start_time = datetime.now()

        # Initialize data preparation pipeline
        data_prep = MLDataPreprocessor(spark=spark)

        # Run full data preparation pipeline
        prepared_df = data_prep.run_full_pipeline(input_date, property_type)

        duration = (datetime.now() - start_time).total_seconds()
        record_count = prepared_df.count()

        logger.logger.info(f"âœ… Data Preparation completed: {duration:.1f}s")
        logger.logger.info(
            f"ğŸ“Š Prepared {record_count:,} records with features for training"
        )
        return True

    except Exception as e:
        logger.log_error(f"âŒ Data Preparation failed: {str(e)}")
        return False


def run_model_training_stage(
    spark: SparkSession, input_date: str, property_type: str, logger
) -> bool:
    """Stage 2: Model Training (Prepared Data â†’ Trained Models)"""
    logger.logger.info("ğŸ¤– Stage 2: Model Training")

    try:
        start_time = datetime.now()

        # Apply performance optimizations to Spark session
        try:
            from data_processing.ml.utils.performance_utils import (
                apply_performance_optimizations,
            )

            optimized_spark = apply_performance_optimizations(spark)
            logger.logger.info("âš¡ Performance optimizations applied")
        except ImportError:
            logger.logger.warning(
                "âš ï¸  Performance utils not found, using default Spark session"
            )
            optimized_spark = spark

        # Create ML trainer instance
        ml_trainer = MLTrainer(spark_session=optimized_spark)

        # Run ML training pipeline
        result = ml_trainer.run_training_pipeline(
            date=input_date, property_type=property_type
        )

        duration = (datetime.now() - start_time).total_seconds()

        # Check if training was successful
        if result and result.get("success", False):
            logger.logger.info(f"âœ… Model Training completed: {duration:.1f}s")
            logger.logger.info(f"ğŸ† Best model: {result.get('best_model', 'N/A')}")
            logger.logger.info(f"ğŸ“Š RÂ²: {result.get('metrics', {}).get('r2', 0):.3f}")
            return True
        else:
            logger.log_error("âŒ Model Training returned unsuccessful result")
            return False

    except Exception as e:
        logger.log_error(f"âŒ Model Training failed: {str(e)}")
        return False


def validate_gold_data_exists(
    spark: SparkSession, input_date: str, property_type: str, logger
) -> bool:
    """Kiá»ƒm tra xem dá»¯ liá»‡u Gold cÃ³ tá»“n táº¡i khÃ´ng"""
    from common.utils.hdfs_utils import check_hdfs_path_exists

    def get_gold_path(property_type: str, date: str) -> str:
        """Táº¡o Ä‘Æ°á»ng dáº«n Gold data Ä‘Æ¡n giáº£n"""
        date_formatted = date.replace("-", "")
        date_path = date.replace("-", "/")
        return f"/data/realestate/processed/gold/unified/{property_type}/{date_path}/unified_{property_type}_{date_formatted}.parquet"

    # Use unified path manager for consistent paths
    gold_path = get_gold_path(property_type, input_date)

    if check_hdfs_path_exists(spark, gold_path):
        logger.logger.info(f"âœ… Gold data found: {gold_path}")
        return True
    else:
        logger.log_error(f"âŒ Gold data not found: {gold_path}")
        logger.logger.error(
            "ğŸ’¡ Suggestion: Run ETL pipeline first to generate Gold data"
        )
        return False


def run_ml_pipeline(
    spark: SparkSession,
    input_date=None,
    property_types=None,
    feature_only=False,
    training_only=False,
    skip_features=False,
    skip_training=False,
    validate_gold=True,
):
    """
    Cháº¡y pipeline ML hoÃ n chá»‰nh

    Args:
        spark: SparkSession
        input_date: NgÃ y xá»­ lÃ½ (YYYY-MM-DD)
        property_types: Danh sÃ¡ch loáº¡i BDS ["house", "other"]
        feature_only: Chá»‰ cháº¡y data preparation
        training_only: Chá»‰ cháº¡y model training
        skip_features: Bá» qua data preparation
        skip_training: Bá» qua model training
        validate_gold: Kiá»ƒm tra dá»¯ liá»‡u Gold trÆ°á»›c khi cháº¡y
    """
    # Default values
    if input_date is None:
        input_date = datetime.now().strftime("%Y-%m-%d")

    if property_types is None:
        property_types = ["house", "other"]

    logger = SparkJobLogger("ml_processing_pipeline")
    logger.start_job(
        {
            "input_date": input_date,
            "property_types": property_types,
            "feature_only": feature_only,
            "training_only": training_only,
        }
    )

    # Validate stage arguments
    if feature_only and training_only:
        logger.log_error(
            "âŒ Error: Cannot specify both --feature-only and --training-only"
        )
        return False

    # Determine which stages to run
    if feature_only:
        stages_to_run = ["data_preparation"]
    elif training_only:
        stages_to_run = ["model_training"]
    else:
        stages_to_run = []
        if not skip_features:
            stages_to_run.append("data_preparation")
        if not skip_training:
            stages_to_run.append("model_training")

    logger.logger.info(f"ğŸš€ Starting ML Pipeline - {input_date}")
    logger.logger.info(f"ğŸ“‹ Property types: {property_types}")
    logger.logger.info(f"ğŸ”§ ML stages to run: {stages_to_run}")

    pipeline_start_time = datetime.now()
    overall_success = True

    try:
        for property_type in property_types:
            logger.logger.info(f"\n{'='*60}")
            logger.logger.info(
                f"ğŸ  Processing ML for Property Type: {property_type.upper()}"
            )
            logger.logger.info(f"{'='*60}")

            property_success = True

            # Stage 1: Data Preparation (Gold â†’ Cleaned Data + Feature Engineering)
            if "data_preparation" in stages_to_run:
                preparation_success = run_data_preparation_stage(
                    spark, input_date, property_type, logger
                )
                if not preparation_success:
                    logger.log_error(f"âŒ Data Preparation failed for {property_type}")
                    property_success = False
                    # Don't continue to training if preparation failed
                    continue

            # Stage 2: Model Training (Prepared Data â†’ Trained Models)
            if "model_training" in stages_to_run:
                # Validate prepared data exists if not just created
                if "data_preparation" not in stages_to_run:
                    # TODO: Add validation for prepared data existence
                    pass

                training_success = run_model_training_stage(
                    spark, input_date, property_type, logger
                )
                if not training_success:
                    logger.log_error(f"âŒ ML Training failed for {property_type}")
                    property_success = False

            if not property_success:
                overall_success = False

    except Exception as e:
        logger.log_error(f"âŒ ML Pipeline failed: {str(e)}")
        overall_success = False

    pipeline_duration = datetime.now() - pipeline_start_time

    if overall_success:
        logger.logger.info(
            f"\nğŸ‰ ML Pipeline completed successfully! Duration: {pipeline_duration}"
        )
    else:
        logger.logger.error(
            f"\nğŸ’¥ ML Pipeline completed with errors! Duration: {pipeline_duration}"
        )

    return overall_success


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="Run ML Processing Pipeline (Gold â†’ Data Preparation â†’ Trained Models)"
    )

    # Basic arguments
    parser.add_argument("--date", type=str, help="Processing date (YYYY-MM-DD)")
    parser.add_argument(
        "--property-types",
        type=str,
        nargs="+",
        default=["house"],
        choices=["house", "other"],
        help="Property types to process",
    )

    # ML stage control arguments - only one of these can be used
    stage_group = parser.add_mutually_exclusive_group()
    stage_group.add_argument(
        "--feature-only",
        action="store_true",
        help="Only run data preparation (Gold â†’ Cleaned Data + Features)",
    )
    stage_group.add_argument(
        "--training-only",
        action="store_true",
        help="Only run model training (Prepared Data â†’ Trained Models)",
    )

    # Skip stage arguments - can be combined
    parser.add_argument(
        "--skip-features",
        action="store_true",
        help="Skip data preparation stage",
    )
    parser.add_argument(
        "--skip-training", action="store_true", help="Skip model training stage"
    )

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    # Create ML-optimized Spark session (khÃ´ng cáº§n config riÃªng ná»¯a)
    spark = create_optimized_ml_spark_session("Real Estate ML Pipeline")

    try:
        # Run the ML pipeline
        success = run_ml_pipeline(
            spark,
            input_date=args.date,
            property_types=args.property_types,
            feature_only=args.feature_only,
            training_only=args.training_only,
            skip_features=args.skip_features,
            skip_training=args.skip_training,
        )

        if success:
            print("âœ… ML Pipeline completed successfully!")
        else:
            print("âŒ ML Pipeline completed with errors!")
            exit(1)

    finally:
        spark.stop()
