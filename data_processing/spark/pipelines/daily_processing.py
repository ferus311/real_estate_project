"""
Pipeline x·ª≠ l√Ω d·ªØ li·ªáu h√†ng ng√†y ƒë∆°n gi·∫£n v√† hi·ªáu qu·∫£
Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold
"""

from pyspark.sql import SparkSession
from datetime import datetime, timedelta
import sys
import os
import argparse

# Th√™m th∆∞ m·ª•c g·ªëc v√†o sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(project_root)

from jobs.extraction.extract_batdongsan import extract_batdongsan_data
from jobs.extraction.extract_chotot import extract_chotot_data
from jobs.transformation.transform_batdongsan import transform_batdongsan_data
from jobs.transformation.transform_chotot import transform_chotot_data
from jobs.transformation.unify_dataset import unify_property_data
from common.utils.date_utils import get_date_format, get_hdfs_path
from common.utils.logging_utils import SparkJobLogger
from common.config.spark_config import create_spark_session

def run_extraction_stage(
    spark: SparkSession, input_date: str, property_type: str, logger
) -> dict:
    """Stage 1: Extraction (Raw ‚Üí Bronze)"""
    logger.logger.info("üîÑ Stage 1: Extraction")

    results = {"batdongsan": False, "chotot": False}

    # Extract Batdongsan
    try:
        start_time = datetime.now()
        batdongsan_df = extract_batdongsan_data(spark, input_date, property_type)
        count = batdongsan_df.count()
        duration = (datetime.now() - start_time).total_seconds()

        logger.logger.info(f"‚úÖ Batdongsan: {count:,} records ({duration:.1f}s)")
        results["batdongsan"] = True

        # Unpersist to free memory
        batdongsan_df.unpersist()

    except Exception as e:
        logger.log_error(f"‚ùå Batdongsan extraction failed: {str(e)}")

    # Extract Chotot
    try:
        start_time = datetime.now()
        chotot_df = extract_chotot_data(spark, input_date, property_type)
        count = chotot_df.count()
        duration = (datetime.now() - start_time).total_seconds()

        logger.logger.info(f"‚úÖ Chotot: {count:,} records ({duration:.1f}s)")
        results["chotot"] = True

        # Unpersist to free memory
        chotot_df.unpersist()

    except Exception as e:
        logger.log_error(f"‚ùå Chotot extraction failed: {str(e)}")

    return results


def run_transformation_stage(
    spark: SparkSession, input_date: str, property_type: str, logger
) -> dict:
    """Stage 2: Transformation (Bronze ‚Üí Silver)"""
    logger.logger.info("üîÑ Stage 2: Transformation")

    results = {"batdongsan": False, "chotot": False}

    # Transform Batdongsan
    try:
        start_time = datetime.now()
        batdongsan_silver_df = transform_batdongsan_data(
            spark, input_date, property_type
        )
        count = batdongsan_silver_df.count()
        duration = (datetime.now() - start_time).total_seconds()

        logger.logger.info(f"‚úÖ Batdongsan Silver: {count:,} records ({duration:.1f}s)")
        results["batdongsan"] = True

        # Unpersist to free memory
        batdongsan_silver_df.unpersist()

    except Exception as e:
        logger.log_error(f"‚ùå Batdongsan transformation failed: {str(e)}")

    # Transform Chotot
    try:
        start_time = datetime.now()
        chotot_silver_df = transform_chotot_data(spark, input_date, property_type)
        count = chotot_silver_df.count()
        duration = (datetime.now() - start_time).total_seconds()

        logger.logger.info(f"‚úÖ Chotot Silver: {count:,} records ({duration:.1f}s)")
        results["chotot"] = True

        # Unpersist to free memory
        chotot_silver_df.unpersist()

    except Exception as e:
        logger.log_error(f"‚ùå Chotot transformation failed: {str(e)}")

    return results


def run_unification_stage(
    spark: SparkSession, input_date: str, property_type: str, logger
) -> bool:
    """Stage 3: Unification (Silver ‚Üí Gold)"""
    logger.logger.info("üîÑ Stage 3: Unification")

    try:
        start_time = datetime.now()
        unified_df = unify_property_data(spark, input_date, property_type)
        count = unified_df.count()
        duration = (datetime.now() - start_time).total_seconds()

        logger.logger.info(f"‚úÖ Unified Gold: {count:,} records ({duration:.1f}s)")

        # Unpersist to free memory
        unified_df.unpersist()

        return True

    except Exception as e:
        logger.log_error(f"‚ùå Unification failed: {str(e)}")
        return False


def run_daily_pipeline(
    spark: SparkSession,
    input_date=None,
    property_types=None,
    extract_only=False,
    transform_only=False,
    unify_only=False,
    skip_extraction=False,
    skip_transformation=False,
    skip_unification=False,
):
    """
    Ch·∫°y pipeline x·ª≠ l√Ω d·ªØ li·ªáu h√†ng ng√†y v·ªõi c√°c t√πy ch·ªçn stage

    Args:
        spark: SparkSession
        input_date: Ng√†y x·ª≠ l√Ω (YYYY-MM-DD)
        property_types: Danh s√°ch lo·∫°i BDS ["house", "other"]
        extract_only: Ch·ªâ ch·∫°y extraction stage
        transform_only: Ch·ªâ ch·∫°y transformation stage
        unify_only: Ch·ªâ ch·∫°y unification stage
        skip_extraction: B·ªè qua extraction stage
        skip_transformation: B·ªè qua transformation stage
        skip_unification: B·ªè qua unification stage
    """
    # Default values
    if input_date is None:
        input_date = datetime.now().strftime("%Y-%m-%d")  # S·ª≠ d·ª•ng ng√†y h√¥m nay

    if property_types is None:
        property_types = ["house", "other"]

    logger = SparkJobLogger("daily_processing_pipeline")
    logger.start_job({"input_date": input_date, "property_types": property_types})

    # Validate stage arguments
    stage_count = sum([extract_only, transform_only, unify_only])
    if stage_count > 1:
        logger.log_error(
            "‚ùå Error: Only one of --extract-only, --transform-only, --unify-only can be specified"
        )
        return False

    # Determine which stages to run
    if extract_only:
        stages_to_run = ["extraction"]
    elif transform_only:
        stages_to_run = ["transformation"]
    elif unify_only:
        stages_to_run = ["unification"]
    else:
        stages_to_run = []
        if not skip_extraction:
            stages_to_run.append("extraction")
        if not skip_transformation:
            stages_to_run.append("transformation")
        if not skip_unification:
            stages_to_run.append("unification")

    logger.logger.info(f"üöÄ Starting Daily Pipeline - {input_date}")
    logger.logger.info(f"üìã Property types: {property_types}")
    logger.logger.info(f"üîß Stages to run: {stages_to_run}")

    pipeline_start_time = datetime.now()
    overall_success = True

    try:
        for property_type in property_types:
            logger.logger.info(f"\n{'='*60}")
            logger.logger.info(f"üè† Processing Property Type: {property_type.upper()}")
            logger.logger.info(f"{'='*60}")

            property_success = True

            # Stage 1: Extraction (Raw ‚Üí Bronze)
            if "extraction" in stages_to_run:
                extraction_results = run_extraction_stage(
                    spark, input_date, property_type, logger
                )
                if not any(extraction_results.values()):
                    logger.log_error(f"‚ùå No data extracted for {property_type}")
                    property_success = False

            # Stage 2: Transformation (Bronze ‚Üí Silver)
            if "transformation" in stages_to_run:
                # Check if bronze data exists before transformation
                if not skip_extraction and "extraction" not in stages_to_run:
                    # Validate bronze data availability using proper HDFS paths
                    bronze_paths = [
                        get_hdfs_path(
                            "/data/realestate/processed/bronze",
                            "batdongsan",
                            property_type,
                            input_date,
                        ),
                        get_hdfs_path(
                            "/data/realestate/processed/bronze",
                            "chotot",
                            property_type,
                            input_date,
                        ),
                    ]
                transformation_results = run_transformation_stage(
                    spark, input_date, property_type, logger
                )
                if not any(transformation_results.values()):
                    logger.log_error(f"‚ùå No data transformed for {property_type}")
                    property_success = False

            # Stage 3: Unification (Silver ‚Üí Gold)
            if "unification" in stages_to_run:
                # Check if silver data exists before unification
                if not skip_transformation and "transformation" not in stages_to_run:
                    # Validate silver data availability using proper HDFS paths
                    silver_paths = [
                        get_hdfs_path(
                            "/data/realestate/processed/silver",
                            "batdongsan",
                            property_type,
                            input_date,
                        ),
                        get_hdfs_path(
                            "/data/realestate/processed/silver",
                            "chotot",
                            property_type,
                            input_date,
                        ),
                    ]

                unification_success = run_unification_stage(
                    spark, input_date, property_type, logger
                )
                if not unification_success:
                    logger.log_error(f"‚ùå Unification failed for {property_type}")
                    property_success = False

            if not property_success:
                overall_success = False

    except Exception as e:
        logger.log_error(f"‚ùå Pipeline failed: {str(e)}")
        overall_success = False

    pipeline_duration = datetime.now() - pipeline_start_time

    if overall_success:
        logger.logger.info(
            f"\nüéâ Pipeline completed successfully! Duration: {pipeline_duration}"
        )
    else:
        logger.logger.error(
            f"\nüí• Pipeline completed with errors! Duration: {pipeline_duration}"
        )

    return overall_success


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="Run Daily Processing Pipeline with Stage Control"
    )

    # Basic arguments
    parser.add_argument("--date", type=str, help="Processing date (YYYY-MM-DD)")
    parser.add_argument(
        "--property-types",
        type=str,
        nargs="+",
        default=["house"],
        choices=["house", "other"],
        help="Property types to process",
    )

    # Stage control arguments - only one of these can be used
    stage_group = parser.add_mutually_exclusive_group()
    stage_group.add_argument(
        "--extract-only",
        action="store_true",
        help="Only run extraction stage (Raw ‚Üí Bronze)",
    )
    stage_group.add_argument(
        "--transform-only",
        action="store_true",
        help="Only run transformation stage (Bronze ‚Üí Silver)",
    )
    stage_group.add_argument(
        "--unify-only",
        action="store_true",
        help="Only run unification stage (Silver ‚Üí Gold)",
    )

    # Skip stage arguments - can be combined
    parser.add_argument(
        "--skip-extraction", action="store_true", help="Skip extraction stage"
    )
    parser.add_argument(
        "--skip-transformation", action="store_true", help="Skip transformation stage"
    )
    parser.add_argument(
        "--skip-unification", action="store_true", help="Skip unification stage"
    )

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    # Create Spark session v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u
    config = {
        "spark.sql.shuffle.partitions": "50",
        "spark.sql.adaptive.enabled": "true",
        "spark.sql.adaptive.coalescePartitions.enabled": "true",
        "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
        "spark.sql.adaptive.skewJoin.enabled": "true",
        "spark.sql.adaptive.localShuffleReader.enabled": "true",
    }

    spark = create_spark_session("Real Estate Daily Pipeline", config=config)

    try:
        success = run_daily_pipeline(
            spark=spark,
            input_date=args.date,
            property_types=args.property_types,
            extract_only=args.extract_only,
            transform_only=args.transform_only,
            unify_only=args.unify_only,
            skip_extraction=args.skip_extraction,
            skip_transformation=args.skip_transformation,
            skip_unification=args.skip_unification,
        )

        if success:
            print("‚úÖ Pipeline completed successfully!")
        else:
            print("‚ùå Pipeline completed with errors!")
            exit(1)

    finally:
        spark.stop()
