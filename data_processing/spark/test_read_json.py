#!/usr/bin/env python3
"""
Test Ä‘á»c JSON file tá»« HDFS Ä‘á»ƒ debug váº¥n Ä‘á»
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

def test_read_json():
    # Táº¡o Spark session Ä‘Æ¡n giáº£n
    spark = SparkSession.builder \
        .appName("Test Read JSON") \
        .config("spark.master", "spark://spark-master:7077") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
        .config("spark.sql.adaptive.enabled", "false") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    print("âœ… Spark session created")
    
    # ÄÆ°á»ng dáº«n test
    test_path = "/data/realestate/raw/batdongsan/house/2025/05/23"
    
    try:
        print(f"ğŸ” Testing path: {test_path}")
        
        # Äá»c JSON Ä‘Æ¡n giáº£n khÃ´ng schema
        print("ğŸ“– Reading JSON without schema...")
        df = spark.read.json(test_path)
        
        print(f"ğŸ¯ Schema inferred:")
        df.printSchema()
        
        print(f"ğŸ“Š Row count: {df.count()}")
        
        print("ğŸ“‹ Sample data:")
        df.show(5, truncate=False)
        
        print("âœ… Test completed successfully!")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        spark.stop()

if __name__ == "__main__":
    test_read_json()
