{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d44f547",
   "metadata": {},
   "source": [
    "# 🔍 Standalone ML Pipeline Investigation\n",
    "\n",
    "## Completely Independent Analysis - No Project Dependencies\n",
    "\n",
    "This notebook investigates why the ML pipeline is filtering out 81,044 out of 81,052 records.\n",
    "\n",
    "### Investigation Goals:\n",
    "1. **Load data directly from HDFS** without any project imports\n",
    "2. **Check available data dates** in the gold layer\n",
    "3. **Analyze data quality** - price, area, missing values\n",
    "4. **Simulate pipeline filtering** step by step\n",
    "5. **Identify root cause** of massive data loss\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057409c",
   "metadata": {},
   "source": [
    "## 1. Setup PySpark - No Project Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a750b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as spark_sum, avg, min as spark_min, max as spark_max,\n",
    "    stddev, isnan, isnull, when, lit, regexp_replace, desc, asc\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, FloatType\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "print(\"✅ Libraries imported successfully - NO PROJECT DEPENDENCIES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac7cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark with HDFS configuration\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"StandaloneInvestigation\")\n",
    "    .config(\"spark.ui.port\", \"4051\")  # Different port to avoid conflicts\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"✅ Spark session initialized: {spark.version}\")\n",
    "print(f\"🌐 Default FS: {spark.conf.get('spark.hadoop.fs.defaultFS')}\")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")  # Reduce log noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8164bcf",
   "metadata": {},
   "source": [
    "## 2. Discover Available Data in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check HDFS connection and discover data structure\n",
    "def check_hdfs_path(path):\n",
    "    \"\"\"Check if HDFS path exists\"\"\"\n",
    "    try:\n",
    "        hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "        fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n",
    "        return fs.exists(spark.sparkContext._jvm.org.apache.hadoop.fs.Path(path))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking path {path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def list_hdfs_directory(path):\n",
    "    \"\"\"List contents of HDFS directory\"\"\"\n",
    "    try:\n",
    "        hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "        fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n",
    "        hdfs_path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(path)\n",
    "\n",
    "        if fs.exists(hdfs_path):\n",
    "            status_list = fs.listStatus(hdfs_path)\n",
    "            return [str(status.getPath().getName()) for status in status_list]\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error listing directory {path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Base path for real estate data\n",
    "base_path = \"/data/realestate/processed/gold/unified\"\n",
    "print(f\"🔍 Checking base path: {base_path}\")\n",
    "\n",
    "if check_hdfs_path(base_path):\n",
    "    print(\"✅ Base path exists\")\n",
    "\n",
    "    # List property types\n",
    "    property_types = list_hdfs_directory(base_path)\n",
    "    print(f\"📁 Available property types: {property_types}\")\n",
    "\n",
    "    # Check house data specifically\n",
    "    house_path = f\"{base_path}/house\"\n",
    "    if check_hdfs_path(house_path):\n",
    "        print(f\"✅ House data path exists: {house_path}\")\n",
    "\n",
    "        # List available years\n",
    "        years = list_hdfs_directory(house_path)\n",
    "        print(f\"📅 Available years: {sorted(years)}\")\n",
    "\n",
    "        # Check recent data\n",
    "        if '2024' in years:\n",
    "            year_path = f\"{house_path}/2024\"\n",
    "            months = list_hdfs_directory(year_path)\n",
    "            print(f\"📅 Available months in 2024: {sorted(months)}\")\n",
    "\n",
    "            if '06' in months:\n",
    "                month_path = f\"{year_path}/06\"\n",
    "                days = list_hdfs_directory(month_path)\n",
    "                print(f\"📅 Available days in 2024-06: {sorted(days)}\")\n",
    "    else:\n",
    "        print(f\"❌ House data path does not exist: {house_path}\")\n",
    "else:\n",
    "    print(f\"❌ Base path does not exist: {base_path}\")\n",
    "\n",
    "    # Try alternative base paths\n",
    "    alternative_bases = [\n",
    "        \"/data/realestate/processed/gold\",\n",
    "        \"/data/realestate/gold\",\n",
    "        \"/data/processed/gold\"\n",
    "    ]\n",
    "\n",
    "    for alt_base in alternative_bases:\n",
    "        print(f\"🔍 Trying alternative base: {alt_base}\")\n",
    "        if check_hdfs_path(alt_base):\n",
    "            print(f\"✅ Found alternative base: {alt_base}\")\n",
    "            contents = list_hdfs_directory(alt_base)\n",
    "            print(f\"📁 Contents: {contents}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fcfe9",
   "metadata": {},
   "source": [
    "## 3. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98113e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load a specific day's data\n",
    "test_date = \"2024-06-07\"\n",
    "property_type = \"house\"\n",
    "\n",
    "# Construct path based on discovered structure\n",
    "data_path = f\"/data/realestate/processed/gold/unified/{property_type}/{test_date.replace('-', '/')}/*.parquet\"\n",
    "print(f\"🔍 Attempting to load data from: {data_path}\")\n",
    "\n",
    "try:\n",
    "    df = spark.read.parquet(data_path)\n",
    "    print(f\"✅ Successfully loaded data from {test_date}\")\n",
    "    print(f\"📊 Total records: {df.count():,}\")\n",
    "    print(f\"📊 Total columns: {len(df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load from primary path: {str(e)}\")\n",
    "\n",
    "    # Try alternative path formats\n",
    "    alternative_paths = [\n",
    "        f\"/data/realestate/processed/gold/{property_type}/{test_date.replace('-', '/')}/*.parquet\",\n",
    "        f\"/data/realestate/gold/{property_type}/{test_date.replace('-', '/')}/*.parquet\",\n",
    "        f\"/data/realestate/processed/gold/unified/{property_type}/unified_*.parquet\",\n",
    "        f\"/data/realestate/processed/gold/{property_type}/unified_*.parquet\"\n",
    "    ]\n",
    "\n",
    "    for alt_path in alternative_paths:\n",
    "        try:\n",
    "            print(f\"🔍 Trying: {alt_path}\")\n",
    "            df = spark.read.parquet(alt_path)\n",
    "            print(f\"✅ SUCCESS! Loaded data from: {alt_path}\")\n",
    "            print(f\"📊 Total records: {df.count():,}\")\n",
    "            print(f\"📊 Total columns: {len(df.columns)}\")\n",
    "            data_path = alt_path\n",
    "            break\n",
    "        except Exception as alt_e:\n",
    "            print(f\"❌ Failed: {str(alt_e)[:100]}...\")\n",
    "    else:\n",
    "        raise Exception(\"Could not find gold data at any expected path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4233fec1",
   "metadata": {},
   "source": [
    "## 4. Analyze Data Schema and Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display complete schema\n",
    "print(\"📋 COMPLETE SCHEMA:\")\n",
    "print(\"=\" * 80)\n",
    "for i, field in enumerate(df.schema.fields, 1):\n",
    "    print(f\"  {i:2d}. {field.name:<20} : {field.dataType.typeName()}\")\n",
    "\n",
    "print(f\"\\n📊 TOTAL COLUMNS: {len(df.columns)}\")\n",
    "print(f\"📊 TOTAL RECORDS: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for critical columns that the ML pipeline needs\n",
    "required_columns = [\n",
    "    'id', 'price', 'area', 'latitude', 'longitude',\n",
    "    'district', 'ward', 'property_type', 'data_date'\n",
    "]\n",
    "\n",
    "print(\"🔍 CRITICAL COLUMNS CHECK:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "available_columns = df.columns\n",
    "for col_name in required_columns:\n",
    "    if col_name in available_columns:\n",
    "        col_type = dict(df.dtypes)[col_name]\n",
    "        print(f\"  ✅ {col_name:<15} : {col_type}\")\n",
    "    else:\n",
    "        print(f\"  ❌ {col_name:<15} : MISSING\")\n",
    "\n",
    "# Check if there are similar column names\n",
    "print(\"\\n🔍 SIMILAR COLUMN NAMES:\")\n",
    "import re\n",
    "for req_col in required_columns:\n",
    "    if req_col not in available_columns:\n",
    "        similar = [col for col in available_columns if req_col.lower() in col.lower() or col.lower() in req_col.lower()]\n",
    "        if similar:\n",
    "            print(f\"  🔍 For '{req_col}' found similar: {similar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data for key columns\n",
    "key_columns = ['id', 'price', 'area', 'latitude', 'longitude'] if all(col in df.columns for col in ['id', 'price', 'area', 'latitude', 'longitude']) else df.columns[:5]\n",
    "\n",
    "print(\"🔍 SAMPLE DATA (First 10 records):\")\n",
    "print(\"=\" * 80)\n",
    "df.select(*key_columns).show(10, truncate=False)\n",
    "\n",
    "# Show data types for these columns\n",
    "print(\"\\n📊 KEY COLUMNS DATA TYPES:\")\n",
    "for col_name in key_columns:\n",
    "    col_type = dict(df.dtypes)[col_name]\n",
    "    print(f\"  - {col_name:<15} : {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0c4f8",
   "metadata": {},
   "source": [
    "## 5. Deep Dive into PRICE Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on price column since that's where most data is being filtered out\n",
    "if 'price' in df.columns:\n",
    "    print(\"💰 PRICE COLUMN DEEP ANALYSIS:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get price column type\n",
    "    price_type = dict(df.dtypes)['price']\n",
    "    print(f\"Price column data type: {price_type}\")\n",
    "\n",
    "    # Total records\n",
    "    total_records = df.count()\n",
    "    print(f\"Total records: {total_records:,}\")\n",
    "\n",
    "    # Null analysis\n",
    "    null_count = df.filter(col('price').isNull()).count()\n",
    "    print(f\"Null prices: {null_count:,} ({(null_count/total_records*100):.2f}%)\")\n",
    "\n",
    "    # If price is string type, check for string issues\n",
    "    if price_type == 'string':\n",
    "        print(\"\\n🔍 STRING PRICE ANALYSIS:\")\n",
    "\n",
    "        # Check for empty strings\n",
    "        empty_count = df.filter(col('price') == '').count()\n",
    "        print(f\"Empty string prices: {empty_count:,} ({(empty_count/total_records*100):.2f}%)\")\n",
    "\n",
    "        # Check for 'null' strings\n",
    "        null_string_count = df.filter(col('price').isin(['null', 'NULL', 'None'])).count()\n",
    "        print(f\"'null' string prices: {null_string_count:,} ({(null_string_count/total_records*100):.2f}%)\")\n",
    "\n",
    "        # Show sample string values\n",
    "        print(\"\\n📝 Sample price string values:\")\n",
    "        sample_prices = df.select('price').filter(col('price').isNotNull() & (col('price') != '')).limit(20).collect()\n",
    "        for i, row in enumerate(sample_prices[:10], 1):\n",
    "            price_val = row['price']\n",
    "            print(f\"  {i:2d}. '{price_val}' (len: {len(str(price_val))})\")\n",
    "\n",
    "        # Try to convert to numeric and see what fails\n",
    "        print(\"\\n🔢 NUMERIC CONVERSION TEST:\")\n",
    "\n",
    "        # Add a column that tries to convert price to double\n",
    "        df_with_numeric = df.withColumn(\n",
    "            'price_numeric',\n",
    "            regexp_replace(col('price'), '[^0-9.]', '').cast(DoubleType())\n",
    "        )\n",
    "\n",
    "        # Count successful conversions\n",
    "        successful_conversions = df_with_numeric.filter(\n",
    "            col('price_numeric').isNotNull() &\n",
    "            (col('price_numeric') > 0)\n",
    "        ).count()\n",
    "\n",
    "        print(f\"Successful numeric conversions: {successful_conversions:,} ({(successful_conversions/total_records*100):.2f}%)\")\n",
    "        print(f\"Failed conversions: {total_records - successful_conversions:,} ({((total_records - successful_conversions)/total_records*100):.2f}%)\")\n",
    "\n",
    "        # Show examples of failed conversions\n",
    "        print(\"\\n❌ Examples of unconvertible prices:\")\n",
    "        failed_prices = df_with_numeric.filter(\n",
    "            col('price').isNotNull() &\n",
    "            (col('price') != '') &\n",
    "            col('price_numeric').isNull()\n",
    "        ).select('price').limit(10).collect()\n",
    "\n",
    "        for i, row in enumerate(failed_prices, 1):\n",
    "            print(f\"  {i}. '{row['price']}'\")\n",
    "\n",
    "        # Show statistics for successful conversions\n",
    "        if successful_conversions > 0:\n",
    "            print(\"\\n📊 Statistics for convertible prices:\")\n",
    "            stats = df_with_numeric.filter(col('price_numeric').isNotNull()).select(\n",
    "                spark_min('price_numeric').alias('min_price'),\n",
    "                spark_max('price_numeric').alias('max_price'),\n",
    "                avg('price_numeric').alias('avg_price'),\n",
    "                count('price_numeric').alias('count_price')\n",
    "            ).collect()[0]\n",
    "\n",
    "            print(f\"  Min: {stats['min_price']:,.0f}\")\n",
    "            print(f\"  Max: {stats['max_price']:,.0f}\")\n",
    "            print(f\"  Average: {stats['avg_price']:,.0f}\")\n",
    "            print(f\"  Count: {stats['count_price']:,}\")\n",
    "\n",
    "    else:\n",
    "        # Price is already numeric\n",
    "        print(\"\\n📊 NUMERIC PRICE ANALYSIS:\")\n",
    "\n",
    "        # Get basic statistics\n",
    "        price_stats = df.select(\n",
    "            spark_min('price').alias('min_price'),\n",
    "            spark_max('price').alias('max_price'),\n",
    "            avg('price').alias('avg_price'),\n",
    "            count('price').alias('count_price')\n",
    "        ).collect()[0]\n",
    "\n",
    "        print(f\"  Min: {price_stats['min_price']:,.2f}\")\n",
    "        print(f\"  Max: {price_stats['max_price']:,.2f}\")\n",
    "        print(f\"  Average: {price_stats['avg_price']:,.2f}\")\n",
    "        print(f\"  Non-null count: {price_stats['count_price']:,}\")\n",
    "\n",
    "        # Check for problematic values\n",
    "        zero_prices = df.filter(col('price') == 0).count()\n",
    "        negative_prices = df.filter(col('price') < 0).count()\n",
    "\n",
    "        print(f\"\\nZero prices: {zero_prices:,}\")\n",
    "        print(f\"Negative prices: {negative_prices:,}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Price column not found in dataset!\")\n",
    "    print(f\"Available columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879df76",
   "metadata": {},
   "source": [
    "## 6. Simulate Pipeline Validation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d5e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the exact validation logic from the ML pipeline\n",
    "print(\"🔍 SIMULATING PIPELINE VALIDATION LOGIC:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Start with the original dataset\n",
    "current_df = df\n",
    "initial_count = current_df.count()\n",
    "print(f\"📊 Starting records: {initial_count:,}\")\n",
    "\n",
    "# Step 1: Basic data validation (similar to _validate_basic_data method)\n",
    "print(\"\\n🔍 Step 1: Basic Data Validation\")\n",
    "\n",
    "# Check what columns exist for validation\n",
    "validation_columns = ['id', 'price', 'area', 'latitude', 'longitude']\n",
    "available_validation_cols = [col for col in validation_columns if col in current_df.columns]\n",
    "print(f\"Available validation columns: {available_validation_cols}\")\n",
    "\n",
    "# Apply validation logic step by step\n",
    "if 'price' in available_validation_cols:\n",
    "    # Check price column type and apply appropriate validation\n",
    "    price_type = dict(current_df.dtypes)['price']\n",
    "    print(f\"Price column type: {price_type}\")\n",
    "\n",
    "    if price_type == 'string':\n",
    "        # String type validation\n",
    "        print(\"\\n💰 Validating STRING price column:\")\n",
    "\n",
    "        # Count records that would be filtered out\n",
    "        null_or_empty = current_df.filter(\n",
    "            col('price').isNull() |\n",
    "            (col('price') == '') |\n",
    "            (col('price') == 'null') |\n",
    "            (col('price') == 'NULL')\n",
    "        ).count()\n",
    "\n",
    "        print(f\"Records with null/empty price: {null_or_empty:,}\")\n",
    "\n",
    "        # Apply the filter (this is what causes the massive data loss!)\n",
    "        current_df = current_df.filter(\n",
    "            col('price').isNotNull() &\n",
    "            (col('price') != '') &\n",
    "            (col('price') != 'null') &\n",
    "            (col('price') != 'NULL')\n",
    "        )\n",
    "\n",
    "        after_price_validation = current_df.count()\n",
    "        filtered_out = initial_count - after_price_validation\n",
    "        print(f\"Records after price validation: {after_price_validation:,}\")\n",
    "        print(f\"Records filtered out: {filtered_out:,} ({(filtered_out/initial_count*100):.2f}%)\")\n",
    "\n",
    "        # ISSUE ANALYSIS: Check if the problem is that price values look like numbers but are stored as strings\n",
    "        print(\"\\n🔍 ANALYZING THE ISSUE:\")\n",
    "\n",
    "        # Let's check the original data before any filtering\n",
    "        print(\"Checking sample price values in original data...\")\n",
    "        original_sample = df.select('price').limit(20).collect()\n",
    "\n",
    "        for i, row in enumerate(original_sample[:10], 1):\n",
    "            price_val = row['price']\n",
    "            is_null = price_val is None\n",
    "            is_empty = price_val == '' if not is_null else False\n",
    "            is_null_string = price_val in ['null', 'NULL'] if not is_null else False\n",
    "\n",
    "            print(f\"  {i:2d}. Value: '{price_val}' | Null: {is_null} | Empty: {is_empty} | NullString: {is_null_string}\")\n",
    "\n",
    "    else:\n",
    "        # Numeric type validation\n",
    "        print(f\"\\n💰 Validating NUMERIC price column ({price_type}):\")\n",
    "\n",
    "        # For numeric columns, only filter null values\n",
    "        null_prices = current_df.filter(col('price').isNull()).count()\n",
    "        print(f\"Records with null price: {null_prices:,}\")\n",
    "\n",
    "        current_df = current_df.filter(col('price').isNotNull())\n",
    "\n",
    "        after_price_validation = current_df.count()\n",
    "        filtered_out = initial_count - after_price_validation\n",
    "        print(f\"Records after price validation: {after_price_validation:,}\")\n",
    "        print(f\"Records filtered out: {filtered_out:,} ({(filtered_out/initial_count*100):.2f}%)\")\n",
    "\n",
    "# Continue with other validations if we still have data\n",
    "if current_df.count() > 0:\n",
    "    print(\"\\n🔍 Step 2: Additional Validations\")\n",
    "\n",
    "    # Area validation\n",
    "    if 'area' in available_validation_cols:\n",
    "        area_type = dict(current_df.dtypes)['area']\n",
    "        print(f\"\\n🏠 Area column type: {area_type}\")\n",
    "\n",
    "        before_area = current_df.count()\n",
    "\n",
    "        if area_type == 'string':\n",
    "            current_df = current_df.filter(\n",
    "                col('area').isNotNull() &\n",
    "                (col('area') != '') &\n",
    "                (col('area') != 'null') &\n",
    "                (col('area') != 'NULL')\n",
    "            )\n",
    "        else:\n",
    "            current_df = current_df.filter(col('area').isNotNull())\n",
    "\n",
    "        after_area = current_df.count()\n",
    "        print(f\"Records after area validation: {after_area:,} (filtered: {before_area - after_area:,})\")\n",
    "\n",
    "    # Location validation\n",
    "    location_cols = ['latitude', 'longitude']\n",
    "    for loc_col in location_cols:\n",
    "        if loc_col in available_validation_cols:\n",
    "            loc_type = dict(current_df.dtypes)[loc_col]\n",
    "            print(f\"\\n📍 {loc_col} column type: {loc_type}\")\n",
    "\n",
    "            before_loc = current_df.count()\n",
    "\n",
    "            if loc_type == 'string':\n",
    "                current_df = current_df.filter(\n",
    "                    col(loc_col).isNotNull() &\n",
    "                    (col(loc_col) != '') &\n",
    "                    (col(loc_col) != 'null') &\n",
    "                    (col(loc_col) != 'NULL')\n",
    "                )\n",
    "            else:\n",
    "                current_df = current_df.filter(col(loc_col).isNotNull())\n",
    "\n",
    "            after_loc = current_df.count()\n",
    "            print(f\"Records after {loc_col} validation: {after_loc:,} (filtered: {before_loc - after_loc:,})\")\n",
    "\n",
    "print(f\"\\n📊 FINAL RESULT:\")\n",
    "final_count = current_df.count()\n",
    "total_filtered = initial_count - final_count\n",
    "print(f\"Final records: {final_count:,}\")\n",
    "print(f\"Total filtered out: {total_filtered:,} ({(total_filtered/initial_count*100):.2f}%)\")\n",
    "\n",
    "if total_filtered > initial_count * 0.8:  # More than 80% filtered\n",
    "    print(\"\\n🚨 CRITICAL ISSUE IDENTIFIED:\")\n",
    "    print(\"More than 80% of data is being filtered out!\")\n",
    "    print(\"This suggests a fundamental data type or validation logic problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb1f78",
   "metadata": {},
   "source": [
    "## 7. Root Cause Analysis and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e10b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 ROOT CAUSE ANALYSIS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check the actual data distribution in key columns\n",
    "print(\"\\n1. DATA TYPE ISSUES:\")\n",
    "\n",
    "# Check if numeric values are stored as strings\n",
    "if 'price' in df.columns:\n",
    "    price_type = dict(df.dtypes)['price']\n",
    "    if price_type == 'string':\n",
    "        print(\"❌ ISSUE: Price is stored as STRING but should be NUMERIC\")\n",
    "\n",
    "        # Test conversion rate\n",
    "        df_test = df.withColumn(\n",
    "            'price_as_double',\n",
    "            regexp_replace(col('price'), '[^0-9.]', '').cast(DoubleType())\n",
    "        )\n",
    "\n",
    "        convertible = df_test.filter(col('price_as_double').isNotNull()).count()\n",
    "        total = df_test.count()\n",
    "\n",
    "        print(f\"   Convertible to numeric: {convertible:,}/{total:,} ({(convertible/total*100):.1f}%)\")\n",
    "\n",
    "        if convertible > total * 0.8:  # More than 80% convertible\n",
    "            print(\"   💡 SOLUTION: Convert string prices to numeric BEFORE validation\")\n",
    "    else:\n",
    "        print(f\"✅ Price column has correct type: {price_type}\")\n",
    "\n",
    "print(\"\\n2. VALIDATION LOGIC ISSUES:\")\n",
    "\n",
    "# Check if the validation is too strict\n",
    "problematic_patterns = {\n",
    "    'empty_strings': df.filter(col('price') == '').count() if 'price' in df.columns else 0,\n",
    "    'null_strings': df.filter(col('price').isin(['null', 'NULL'])).count() if 'price' in df.columns else 0,\n",
    "    'actual_nulls': df.filter(col('price').isNull()).count() if 'price' in df.columns else 0\n",
    "}\n",
    "\n",
    "total_records = df.count()\n",
    "for pattern, count in problematic_patterns.items():\n",
    "    if count > 0:\n",
    "        pct = (count / total_records) * 100\n",
    "        print(f\"   {pattern}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "print(\"\\n3. RECOMMENDATIONS:\")\n",
    "print(\"   1. 🔧 Add data type optimization BEFORE validation\")\n",
    "print(\"   2. 🔧 Convert string numeric columns to proper types\")\n",
    "print(\"   3. 🔧 Use type-aware validation logic\")\n",
    "print(\"   4. 🔧 Add better logging to track filtering steps\")\n",
    "print(\"   5. 🔧 Consider relaxing validation criteria for development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d76e827",
   "metadata": {},
   "source": [
    "## 8. Proposed Fix Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ad318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 TESTING PROPOSED FIX:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Implement the fix that should be applied to the pipeline\n",
    "def optimize_data_types(df):\n",
    "    \"\"\"Convert string columns to appropriate numeric types\"\"\"\n",
    "\n",
    "    # Define columns that should be numeric\n",
    "    numeric_columns = ['price', 'area', 'latitude', 'longitude', 'price_per_m2']\n",
    "\n",
    "    result_df = df\n",
    "    conversions_applied = []\n",
    "\n",
    "    for col_name in numeric_columns:\n",
    "        if col_name in df.columns:\n",
    "            current_type = dict(df.dtypes)[col_name]\n",
    "\n",
    "            if current_type == 'string':\n",
    "                print(f\"🔄 Converting {col_name} from string to double\")\n",
    "\n",
    "                # Clean and convert\n",
    "                result_df = result_df.withColumn(\n",
    "                    col_name,\n",
    "                    regexp_replace(col(col_name), '[^0-9.-]', '').cast(DoubleType())\n",
    "                )\n",
    "\n",
    "                conversions_applied.append(col_name)\n",
    "\n",
    "    print(f\"✅ Applied conversions to: {conversions_applied}\")\n",
    "    return result_df\n",
    "\n",
    "def validate_with_proper_types(df):\n",
    "    \"\"\"Apply validation logic that's aware of data types\"\"\"\n",
    "\n",
    "    result_df = df\n",
    "    initial_count = df.count()\n",
    "\n",
    "    print(f\"Starting validation with {initial_count:,} records\")\n",
    "\n",
    "    validation_columns = ['price', 'area', 'latitude', 'longitude']\n",
    "\n",
    "    for col_name in validation_columns:\n",
    "        if col_name in df.columns:\n",
    "            col_type = dict(result_df.dtypes)[col_name]\n",
    "            before_count = result_df.count()\n",
    "\n",
    "            if col_type in ['double', 'float', 'int', 'integer', 'long']:\n",
    "                # Numeric validation - only filter nulls and invalid values\n",
    "                if col_name in ['latitude', 'longitude']:\n",
    "                    # For coordinates, filter null and extreme values\n",
    "                    result_df = result_df.filter(\n",
    "                        col(col_name).isNotNull() &\n",
    "                        (col(col_name) != 0) &  # Invalid coordinates\n",
    "                        (col(col_name).between(-180, 180))\n",
    "                    )\n",
    "                elif col_name in ['price', 'area']:\n",
    "                    # For price and area, filter null and negative/zero values\n",
    "                    result_df = result_df.filter(\n",
    "                        col(col_name).isNotNull() &\n",
    "                        (col(col_name) > 0)\n",
    "                    )\n",
    "                else:\n",
    "                    # General numeric validation\n",
    "                    result_df = result_df.filter(col(col_name).isNotNull())\n",
    "\n",
    "            else:\n",
    "                # String validation - filter null, empty, and 'null' strings\n",
    "                result_df = result_df.filter(\n",
    "                    col(col_name).isNotNull() &\n",
    "                    (col(col_name) != '') &\n",
    "                    (~col(col_name).isin(['null', 'NULL', 'None']))\n",
    "                )\n",
    "\n",
    "            after_count = result_df.count()\n",
    "            filtered = before_count - after_count\n",
    "\n",
    "            print(f\"  {col_name} ({col_type}): {after_count:,} records (filtered: {filtered:,})\")\n",
    "\n",
    "    final_count = result_df.count()\n",
    "    total_filtered = initial_count - final_count\n",
    "\n",
    "    print(f\"\\n📊 Validation complete:\")\n",
    "    print(f\"  Final records: {final_count:,}\")\n",
    "    print(f\"  Total filtered: {total_filtered:,} ({(total_filtered/initial_count*100):.1f}%)\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Apply the fix\n",
    "print(\"\\n🔧 STEP 1: Optimize Data Types\")\n",
    "fixed_df = optimize_data_types(df)\n",
    "\n",
    "# Show the difference\n",
    "print(\"\\n📋 Data types after optimization:\")\n",
    "key_columns = ['price', 'area', 'latitude', 'longitude']\n",
    "for col_name in key_columns:\n",
    "    if col_name in fixed_df.columns:\n",
    "        original_type = dict(df.dtypes)[col_name]\n",
    "        new_type = dict(fixed_df.dtypes)[col_name]\n",
    "        print(f\"  {col_name}: {original_type} → {new_type}\")\n",
    "\n",
    "print(\"\\n🔧 STEP 2: Apply Proper Validation\")\n",
    "validated_df = validate_with_proper_types(fixed_df)\n",
    "\n",
    "# Compare with original broken validation\n",
    "original_count = df.count()\n",
    "fixed_count = validated_df.count()\n",
    "improvement = fixed_count - final_count  # final_count from previous broken validation\n",
    "\n",
    "print(f\"\\n📊 COMPARISON:\")\n",
    "print(f\"  Original data: {original_count:,} records\")\n",
    "print(f\"  Broken pipeline result: {final_count:,} records ({(final_count/original_count*100):.1f}% retained)\")\n",
    "print(f\"  Fixed pipeline result: {fixed_count:,} records ({(fixed_count/original_count*100):.1f}% retained)\")\n",
    "print(f\"  Improvement: +{improvement:,} records ({(improvement/original_count*100):.1f}% better retention)\")\n",
    "\n",
    "if fixed_count > original_count * 0.5:  # Retaining more than 50%\n",
    "    print(\"\\n✅ SUCCESS: Fix dramatically improves data retention!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  WARNING: Still losing significant data - further investigation needed\")\n",
    "\n",
    "# Show sample of final data\n",
    "if fixed_count > 0:\n",
    "    print(\"\\n🔍 Sample of cleaned data:\")\n",
    "    sample_cols = [col for col in ['id', 'price', 'area', 'latitude', 'longitude'] if col in validated_df.columns]\n",
    "    validated_df.select(*sample_cols).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cff521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"✅ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
