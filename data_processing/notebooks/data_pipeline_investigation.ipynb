{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2461aa8c",
   "metadata": {},
   "source": [
    "# üîç ML Pipeline Data Investigation\n",
    "\n",
    "## Ph√¢n t√≠ch t·∫°i sao pipeline lo·∫°i b·ªè qu√° nhi·ªÅu d·ªØ li·ªáu\n",
    "\n",
    "D·ª±a tr√™n log, pipeline ƒëang lo·∫°i b·ªè r·∫•t nhi·ªÅu d·ªØ li·ªáu:\n",
    "- **B·∫Øt ƒë·∫ßu**: 81,052 records\n",
    "- **Sau validation price**: 8 records (lo·∫°i b·ªè 81,044 records!)\n",
    "- **Cu·ªëi c√πng**: 6 records\n",
    "\n",
    "### M·ª•c ti√™u ƒëi·ªÅu tra:\n",
    "1. **Price range validation**: T·∫°i sao 81,044 records b·ªã lo·∫°i b·ªè do \"invalid price\"?\n",
    "2. **Data types**: Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu c·ªßa c√°c c·ªôt quan tr·ªçng\n",
    "3. **Value distributions**: Ph√¢n t√≠ch ph√¢n ph·ªëi gi√° tr·ªã price v√† area\n",
    "4. **Missing values**: T√¨m hi·ªÉu t·ª∑ l·ªá missing values\n",
    "5. **Validation logic**: Xem x√©t logic validation c√≥ ph√π h·ª£p kh√¥ng\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ff32f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2c5dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Add project paths\n",
    "sys.path.append('/home/fer/data/real_estate_project/data_processing')\n",
    "sys.path.append('/home/fer/data/real_estate_project/data_processing/common')\n",
    "sys.path.append('/home/fer/data/real_estate_project/data_processing/ml')\n",
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as spark_sum, avg, min as spark_min, max as spark_max,\n",
    "    stddev, isnan, isnull, when, lit, regexp_replace\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6979b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/09 10:23:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session initialized: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"DataPipelineInvestigation\")\n",
    "    .config(\"spark.ui.port\", \"4050\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Spark session initialized: {spark.version}\")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")  # Reduce log noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4f46d",
   "metadata": {},
   "source": [
    "## 2. Load Gold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47005fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Loading data from 2024-05-09 to 2024-06-07\n",
      "üè† Property type: house\n",
      "üîç Sample path: hdfs://namenode:9000/data/realestate/processed/gold/unified/house/2024/06/07/*.parquet\n",
      "‚ùå Failed to load sample data: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/data/realestate/processed/gold/unified/house/2024/06/07/*.parquet.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Could not find gold data at any expected path",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Load sample data\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     sample_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgold_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Successfully loaded sample data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:531\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    522\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    523\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    529\u001b[0m )\n\u001b[0;32m--> 531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/data/realestate/processed/gold/unified/house/2024/06/07/*.parquet.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find gold data at any expected path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Could not find gold data at any expected path"
     ]
    }
   ],
   "source": [
    "# Set up parameters (same as in pipeline)\n",
    "property_type = \"house\"\n",
    "date = \"2024-06-07\"  # Test date\n",
    "lookback_days = 30\n",
    "\n",
    "# Calculate date range\n",
    "end_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "start_date = end_date - timedelta(days=lookback_days - 1)\n",
    "\n",
    "print(f\"üìÖ Loading data from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"üè† Property type: {property_type}\")\n",
    "\n",
    "# Try to load one day of data first to understand the structure\n",
    "sample_date = \"2024-06-07\"\n",
    "date_formatted = sample_date.replace(\"-\", \"\")\n",
    "gold_path = f\"hdfs://namenode:9000/data/realestate/processed/gold/unified/{property_type}/{sample_date.replace('-', '/')}/*.parquet\"\n",
    "\n",
    "print(f\"üîç Sample path: {gold_path}\")\n",
    "\n",
    "try:\n",
    "    # Load sample data\n",
    "    sample_df = spark.read.parquet(gold_path)\n",
    "    print(f\"‚úÖ Successfully loaded sample data from {sample_date}\")\n",
    "    print(f\"üìä Sample records: {sample_df.count():,}\")\n",
    "    print(f\"üìä Sample columns: {len(sample_df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load sample data: {str(e)}\")\n",
    "\n",
    "    # Try different path formats\n",
    "    alternative_paths = [\n",
    "        f\"/data/realestate/processed/gold/unified/{property_type}/unified_*.parquet\",\n",
    "        f\"/data/realestate/processed/gold/{property_type}/{sample_date.replace('-', '/')}/*.parquet\",\n",
    "        f\"/data/realestate/processed/gold/{property_type}/unified_{property_type}_{date_formatted}.parquet\"\n",
    "    ]\n",
    "\n",
    "    for alt_path in alternative_paths:\n",
    "        try:\n",
    "            sample_df = spark.read.parquet(alt_path)\n",
    "            print(f\"‚úÖ Found data at alternative path: {alt_path}\")\n",
    "            print(f\"üìä Records: {sample_df.count():,}\")\n",
    "            gold_path = alt_path\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        raise Exception(\"Could not find gold data at any expected path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d565006",
   "metadata": {},
   "source": [
    "## 3. Inspect Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cf69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema\n",
    "print(\"üìã SCHEMA INFORMATION:\")\n",
    "print(\"=\" * 50)\n",
    "for field in sample_df.schema.fields:\n",
    "    print(f\"  - {field.name}: {field.dataType.typeName()}\")\n",
    "\n",
    "print(f\"\\nüìä TOTAL COLUMNS: {len(sample_df.columns)}\")\n",
    "print(f\"üìä TOTAL RECORDS: {sample_df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae1aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data for critical columns\n",
    "critical_columns = ['id', 'price', 'area', 'latitude', 'longitude', 'district', 'ward']\n",
    "available_critical = [col for col in critical_columns if col in sample_df.columns]\n",
    "\n",
    "print(\"\\nüîç SAMPLE DATA (First 10 records):\")\n",
    "print(\"=\" * 80)\n",
    "sample_data = sample_df.select(*available_critical).limit(10)\n",
    "sample_data.show(truncate=False)\n",
    "\n",
    "# Show data types for these columns\n",
    "print(\"\\nüìä CRITICAL COLUMNS DATA TYPES:\")\n",
    "for col_name in available_critical:\n",
    "    col_type = dict(sample_df.dtypes)[col_name]\n",
    "    print(f\"  - {col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f00da",
   "metadata": {},
   "source": [
    "## 4. Validate Critical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values and data quality\n",
    "print(\"üîç NULL VALUE ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_records = sample_df.count()\n",
    "print(f\"Total records: {total_records:,}\\n\")\n",
    "\n",
    "for col_name in available_critical:\n",
    "    null_count = sample_df.filter(col(col_name).isNull()).count()\n",
    "    null_pct = (null_count / total_records) * 100 if total_records > 0 else 0\n",
    "\n",
    "    print(f\"{col_name}:\")\n",
    "    print(f\"  - Null values: {null_count:,} ({null_pct:.2f}%)\")\n",
    "\n",
    "    # For string columns, also check empty strings\n",
    "    col_type = dict(sample_df.dtypes)[col_name]\n",
    "    if col_type == 'string':\n",
    "        empty_count = sample_df.filter((col(col_name) == '') | (col(col_name) == 'null') | (col(col_name) == 'NULL')).count()\n",
    "        empty_pct = (empty_count / total_records) * 100 if total_records > 0 else 0\n",
    "        print(f\"  - Empty/null strings: {empty_count:,} ({empty_pct:.2f}%)\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ede7b",
   "metadata": {},
   "source": [
    "## 5. Investigate Invalid Price Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d724d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze price column in detail\n",
    "if 'price' in sample_df.columns:\n",
    "    print(\"üí∞ PRICE ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Get price column type\n",
    "    price_type = dict(sample_df.dtypes)['price']\n",
    "    print(f\"Price column data type: {price_type}\")\n",
    "\n",
    "    # Show sample price values\n",
    "    print(\"\\nüîç Sample price values:\")\n",
    "    sample_prices = sample_df.select('price').limit(20).collect()\n",
    "    for i, row in enumerate(sample_prices):\n",
    "        print(f\"  {i+1}: {row['price']} (type: {type(row['price'])})\")\n",
    "\n",
    "    # Get basic statistics\n",
    "    print(\"\\nüìä Price statistics:\")\n",
    "    price_stats = sample_df.select(\n",
    "        spark_min('price').alias('min_price'),\n",
    "        spark_max('price').alias('max_price'),\n",
    "        avg('price').alias('avg_price'),\n",
    "        count('price').alias('count_price')\n",
    "    ).collect()[0]\n",
    "\n",
    "    print(f\"  - Min: {price_stats['min_price']}\")\n",
    "    print(f\"  - Max: {price_stats['max_price']}\")\n",
    "    print(f\"  - Average: {price_stats['avg_price']:,.2f}\" if price_stats['avg_price'] else \"N/A\")\n",
    "    print(f\"  - Non-null count: {price_stats['count_price']:,}\")\n",
    "\n",
    "    # Check for zero and negative values\n",
    "    zero_count = sample_df.filter(col('price') == 0).count()\n",
    "    negative_count = sample_df.filter(col('price') < 0).count()\n",
    "    print(f\"\\n‚ö†Ô∏è Price issues:\")\n",
    "    print(f\"  - Zero values: {zero_count:,}\")\n",
    "    print(f\"  - Negative values: {negative_count:,}\")\n",
    "\n",
    "    # Test the actual pipeline validation logic\n",
    "    print(\"\\nüß™ TESTING PIPELINE VALIDATION LOGIC:\")\n",
    "    print(\"Pipeline filters: price > 100,000 AND price < 100,000,000,000\")\n",
    "\n",
    "    valid_price_count = sample_df.filter(\n",
    "        (col('price') > 100000) &\n",
    "        (col('price') < 100000000000) &\n",
    "        (col('price').isNotNull())\n",
    "    ).count()\n",
    "\n",
    "    invalid_price_count = total_records - valid_price_count\n",
    "\n",
    "    print(f\"  - Records passing validation: {valid_price_count:,}\")\n",
    "    print(f\"  - Records failing validation: {invalid_price_count:,}\")\n",
    "    print(f\"  - Failure rate: {(invalid_price_count/total_records)*100:.2f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Price column not found in data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze price ranges in detail\n",
    "if 'price' in sample_df.columns:\n",
    "    print(\"\\nüìà PRICE RANGE ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Count records in different price ranges\n",
    "    price_ranges = [\n",
    "        (\"< 100K\", col('price') < 100000),\n",
    "        (\"100K - 1M\", (col('price') >= 100000) & (col('price') < 1000000)),\n",
    "        (\"1M - 10M\", (col('price') >= 1000000) & (col('price') < 10000000)),\n",
    "        (\"10M - 100M\", (col('price') >= 10000000) & (col('price') < 100000000)),\n",
    "        (\"100M - 1B\", (col('price') >= 100000000) & (col('price') < 1000000000)),\n",
    "        (\"> 1B\", col('price') >= 1000000000),\n",
    "        (\"Null/Invalid\", col('price').isNull())\n",
    "    ]\n",
    "\n",
    "    total_non_null = sample_df.filter(col('price').isNotNull()).count()\n",
    "\n",
    "    for range_name, condition in price_ranges:\n",
    "        count = sample_df.filter(condition).count()\n",
    "        pct = (count / total_records) * 100 if total_records > 0 else 0\n",
    "        print(f\"  {range_name}: {count:,} records ({pct:.2f}%)\")\n",
    "\n",
    "    # Show examples of records being filtered out\n",
    "    print(\"\\nüîç EXAMPLES OF FILTERED OUT RECORDS:\")\n",
    "    filtered_out = sample_df.filter(\n",
    "        (col('price') <= 100000) |\n",
    "        (col('price') >= 100000000000) |\n",
    "        col('price').isNull()\n",
    "    ).select('id', 'price', 'area', 'district').limit(10)\n",
    "\n",
    "    if filtered_out.count() > 0:\n",
    "        filtered_out.show(truncate=False)\n",
    "    else:\n",
    "        print(\"  No records would be filtered out with current validation logic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea2fdd4",
   "metadata": {},
   "source": [
    "## 6. Investigate Invalid Area Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze area column in detail\n",
    "if 'area' in sample_df.columns:\n",
    "    print(\"üìê AREA ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Get area column type\n",
    "    area_type = dict(sample_df.dtypes)['area']\n",
    "    print(f\"Area column data type: {area_type}\")\n",
    "\n",
    "    # Show sample area values\n",
    "    print(\"\\nüîç Sample area values:\")\n",
    "    sample_areas = sample_df.select('area').limit(20).collect()\n",
    "    for i, row in enumerate(sample_areas):\n",
    "        print(f\"  {i+1}: {row['area']} (type: {type(row['area'])})\")\n",
    "\n",
    "    # Get basic statistics\n",
    "    print(\"\\nüìä Area statistics:\")\n",
    "    area_stats = sample_df.select(\n",
    "        spark_min('area').alias('min_area'),\n",
    "        spark_max('area').alias('max_area'),\n",
    "        avg('area').alias('avg_area'),\n",
    "        count('area').alias('count_area')\n",
    "    ).collect()[0]\n",
    "\n",
    "    print(f\"  - Min: {area_stats['min_area']}\")\n",
    "    print(f\"  - Max: {area_stats['max_area']}\")\n",
    "    print(f\"  - Average: {area_stats['avg_area']:,.2f}\" if area_stats['avg_area'] else \"N/A\")\n",
    "    print(f\"  - Non-null count: {area_stats['count_area']:,}\")\n",
    "\n",
    "    # Check for zero and negative values\n",
    "    zero_count = sample_df.filter(col('area') == 0).count()\n",
    "    negative_count = sample_df.filter(col('area') < 0).count()\n",
    "    print(f\"\\n‚ö†Ô∏è Area issues:\")\n",
    "    print(f\"  - Zero values: {zero_count:,}\")\n",
    "    print(f\"  - Negative values: {negative_count:,}\")\n",
    "\n",
    "    # Test pipeline validation logic for area\n",
    "    print(\"\\nüß™ TESTING AREA VALIDATION LOGIC:\")\n",
    "    print(\"Pipeline filters: area > 5 AND area < 50,000\")\n",
    "\n",
    "    valid_area_count = sample_df.filter(\n",
    "        (col('area') > 5) &\n",
    "        (col('area') < 50000) &\n",
    "        (col('area').isNotNull())\n",
    "    ).count()\n",
    "\n",
    "    invalid_area_count = total_records - valid_area_count\n",
    "\n",
    "    print(f\"  - Records passing validation: {valid_area_count:,}\")\n",
    "    print(f\"  - Records failing validation: {invalid_area_count:,}\")\n",
    "    print(f\"  - Failure rate: {(invalid_area_count/total_records)*100:.2f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Area column not found in data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47977a64",
   "metadata": {},
   "source": [
    "## 7. Analyze Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive missing value analysis\n",
    "print(\"üìä MISSING VALUES ANALYSIS (All Columns):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "missing_analysis = []\n",
    "\n",
    "for col_name in sample_df.columns:\n",
    "    # Count different types of missing values\n",
    "    null_count = sample_df.filter(col(col_name).isNull()).count()\n",
    "\n",
    "    col_type = dict(sample_df.dtypes)[col_name]\n",
    "\n",
    "    if col_type == 'string':\n",
    "        # For string columns, also count empty strings and 'null' strings\n",
    "        empty_count = sample_df.filter(\n",
    "            (col(col_name) == '') |\n",
    "            (col(col_name) == 'null') |\n",
    "            (col(col_name) == 'NULL') |\n",
    "            (col(col_name) == 'None')\n",
    "        ).count()\n",
    "        total_missing = null_count + empty_count\n",
    "    else:\n",
    "        # For numeric columns, only count nulls\n",
    "        total_missing = null_count\n",
    "\n",
    "    missing_pct = (total_missing / total_records) * 100 if total_records > 0 else 0\n",
    "\n",
    "    missing_analysis.append({\n",
    "        'column': col_name,\n",
    "        'type': col_type,\n",
    "        'null_count': null_count,\n",
    "        'total_missing': total_missing,\n",
    "        'missing_pct': missing_pct\n",
    "    })\n",
    "\n",
    "# Sort by missing percentage\n",
    "missing_analysis.sort(key=lambda x: x['missing_pct'], reverse=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"{'Column':<20} {'Type':<10} {'Missing':<10} {'Percentage':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for item in missing_analysis:\n",
    "    print(f\"{item['column']:<20} {item['type']:<10} {item['total_missing']:<10,} {item['missing_pct']:<12.2f}%\")\n",
    "\n",
    "# Highlight columns with high missing rates\n",
    "print(\"\\n‚ö†Ô∏è COLUMNS WITH >30% MISSING VALUES:\")\n",
    "high_missing = [item for item in missing_analysis if item['missing_pct'] > 30]\n",
    "if high_missing:\n",
    "    for item in high_missing:\n",
    "        print(f\"  - {item['column']}: {item['missing_pct']:.1f}% missing\")\n",
    "else:\n",
    "    print(\"  None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e93e19",
   "metadata": {},
   "source": [
    "## 8. Investigate Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07792ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier analysis using IQR method (same as pipeline)\n",
    "print(\"üìä OUTLIER ANALYSIS (IQR Method):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "numeric_columns = ['price', 'area']\n",
    "if 'price_per_m2' in sample_df.columns:\n",
    "    numeric_columns.append('price_per_m2')\n",
    "\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in sample_df.columns:\n",
    "        print(f\"\\nüîç Analyzing outliers in {col_name}:\")\n",
    "\n",
    "        # Calculate quantiles (same as pipeline)\n",
    "        quantiles = sample_df.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "\n",
    "        if len(quantiles) == 2:\n",
    "            q1, q3 = quantiles\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "            print(f\"  - Q1: {q1:,.2f}\")\n",
    "            print(f\"  - Q3: {q3:,.2f}\")\n",
    "            print(f\"  - IQR: {iqr:,.2f}\")\n",
    "            print(f\"  - Lower bound: {lower_bound:,.2f}\")\n",
    "            print(f\"  - Upper bound: {upper_bound:,.2f}\")\n",
    "\n",
    "            # Count outliers\n",
    "            total_non_null = sample_df.filter(col(col_name).isNotNull()).count()\n",
    "\n",
    "            outliers_count = sample_df.filter(\n",
    "                (col(col_name) < lower_bound) |\n",
    "                (col(col_name) > upper_bound)\n",
    "            ).count()\n",
    "\n",
    "            valid_count = sample_df.filter(\n",
    "                (col(col_name) >= lower_bound) &\n",
    "                (col(col_name) <= upper_bound) &\n",
    "                col(col_name).isNotNull()\n",
    "            ).count()\n",
    "\n",
    "            outlier_pct = (outliers_count / total_non_null) * 100 if total_non_null > 0 else 0\n",
    "\n",
    "            print(f\"  - Total non-null records: {total_non_null:,}\")\n",
    "            print(f\"  - Outliers: {outliers_count:,} ({outlier_pct:.2f}%)\")\n",
    "            print(f\"  - Records after outlier removal: {valid_count:,}\")\n",
    "\n",
    "            # Show examples of outliers\n",
    "            print(f\"\\n  üìã Examples of outliers:\")\n",
    "            outlier_examples = sample_df.filter(\n",
    "                (col(col_name) < lower_bound) | (col(col_name) > upper_bound)\n",
    "            ).select('id', col_name, 'area' if col_name != 'area' else 'price').limit(5)\n",
    "\n",
    "            if outlier_examples.count() > 0:\n",
    "                outlier_examples.show()\n",
    "            else:\n",
    "                print(\"    No outliers found\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Could not calculate quantiles for {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46f7a2",
   "metadata": {},
   "source": [
    "## 9. Summary of Data Removal Reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeda210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis of data removal pipeline\n",
    "print(\"üìã COMPREHENSIVE DATA REMOVAL ANALYSIS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"Starting records: {total_records:,}\")\n",
    "print(\"\\nüîç Step-by-step filtering simulation:\")\n",
    "\n",
    "# Step 1: Remove duplicates (simulate)\n",
    "current_df = sample_df\n",
    "print(f\"\\n1Ô∏è‚É£ After duplicate removal: {current_df.count():,} records\")\n",
    "\n",
    "# Step 2: Critical column validation\n",
    "print(\"\\n2Ô∏è‚É£ Critical column validation:\")\n",
    "critical_columns = ['price', 'area', 'latitude', 'longitude']\n",
    "\n",
    "for col_name in critical_columns:\n",
    "    if col_name in current_df.columns:\n",
    "        before_count = current_df.count()\n",
    "\n",
    "        col_type = dict(current_df.dtypes)[col_name]\n",
    "\n",
    "        if col_type == 'string':\n",
    "            # String validation\n",
    "            current_df = current_df.filter(\n",
    "                col(col_name).isNotNull() &\n",
    "                (col(col_name) != '') &\n",
    "                (col(col_name) != 'null') &\n",
    "                (col(col_name) != 'NULL')\n",
    "            )\n",
    "        else:\n",
    "            # Numeric validation\n",
    "            if col_name == 'area':\n",
    "                current_df = current_df.filter(col(col_name).isNotNull() & (col(col_name) >= 0))\n",
    "            else:\n",
    "                current_df = current_df.filter(col(col_name).isNotNull() & (col(col_name) > 0))\n",
    "\n",
    "        after_count = current_df.count()\n",
    "        removed = before_count - after_count\n",
    "        print(f\"   {col_name}: {before_count:,} ‚Üí {after_count:,} (-{removed:,} records)\")\n",
    "\n",
    "# Step 3: Numeric range validation\n",
    "print(\"\\n3Ô∏è‚É£ Numeric range validation:\")\n",
    "\n",
    "# Price validation\n",
    "if 'price' in current_df.columns:\n",
    "    before_count = current_df.count()\n",
    "    current_df = current_df.filter(\n",
    "        (col('price') > 100000) &\n",
    "        (col('price') < 100000000000) &\n",
    "        col('price').isNotNull()\n",
    "    )\n",
    "    after_count = current_df.count()\n",
    "    removed = before_count - after_count\n",
    "    print(f\"   Price range (100K-100B): {before_count:,} ‚Üí {after_count:,} (-{removed:,} records)\")\n",
    "\n",
    "# Area validation\n",
    "if 'area' in current_df.columns:\n",
    "    before_count = current_df.count()\n",
    "    current_df = current_df.filter(\n",
    "        (col('area') > 5) &\n",
    "        (col('area') < 50000) &\n",
    "        col('area').isNotNull()\n",
    "    )\n",
    "    after_count = current_df.count()\n",
    "    removed = before_count - after_count\n",
    "    print(f\"   Area range (5-50K sqm): {before_count:,} ‚Üí {after_count:,} (-{removed:,} records)\")\n",
    "\n",
    "# Step 4: Missing value handling (columns with >30% missing)\n",
    "print(\"\\n4Ô∏è‚É£ Missing value column removal:\")\n",
    "high_missing_cols = [item['column'] for item in missing_analysis if item['missing_pct'] > 30]\n",
    "if high_missing_cols:\n",
    "    print(f\"   Dropping columns: {high_missing_cols}\")\n",
    "    current_df = current_df.drop(*high_missing_cols)\n",
    "    print(f\"   Remaining columns: {len(current_df.columns)}\")\n",
    "else:\n",
    "    print(\"   No columns to drop (none have >30% missing)\")\n",
    "\n",
    "# Step 5: Outlier removal\n",
    "print(\"\\n5Ô∏è‚É£ Outlier removal (IQR method):\")\n",
    "outlier_columns = ['price', 'area']\n",
    "for col_name in outlier_columns:\n",
    "    if col_name in current_df.columns:\n",
    "        before_count = current_df.count()\n",
    "\n",
    "        quantiles = current_df.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "        if len(quantiles) == 2:\n",
    "            q1, q3 = quantiles\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "            current_df = current_df.filter(\n",
    "                (col(col_name) >= lower_bound) &\n",
    "                (col(col_name) <= upper_bound)\n",
    "            )\n",
    "\n",
    "            after_count = current_df.count()\n",
    "            removed = before_count - after_count\n",
    "            print(f\"   {col_name} outliers: {before_count:,} ‚Üí {after_count:,} (-{removed:,} records)\")\n",
    "\n",
    "final_count = current_df.count()\n",
    "print(f\"\\nüéØ FINAL RESULT: {final_count:,} records\")\n",
    "print(f\"üìä Total removed: {total_records - final_count:,} records ({((total_records - final_count)/total_records)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed8b3bd",
   "metadata": {},
   "source": [
    "## üéØ K·∫øt lu·∫≠n v√† Khuy·∫øn ngh·ªã\n",
    "\n",
    "### V·∫•n ƒë·ªÅ ch√≠nh ƒë∆∞·ª£c ph√°t hi·ªán:\n",
    "\n",
    "1. **Price Range Validation qu√° nghi√™m ng·∫∑t**: \n",
    "   - Pipeline filter `price > 100,000 VND` c√≥ th·ªÉ lo·∫°i b·ªè nhi·ªÅu d·ªØ li·ªáu h·ª£p l·ªá\n",
    "   - Trong th·ªã tr∆∞·ªùng BDS Vi·ªát Nam, gi√° c√≥ th·ªÉ th·∫•p h∆°n 100k VND cho m·ªôt s·ªë tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát\n",
    "\n",
    "2. **Data Type Issues**:\n",
    "   - C·∫ßn ki·ªÉm tra xem d·ªØ li·ªáu c√≥ ƒëang ·ªü d·∫°ng string v√† c·∫ßn convert kh√¥ng\n",
    "   - Price c√≥ th·ªÉ ƒëang stored ·ªü d·∫°ng kh√¥ng mong ƒë·ª£i\n",
    "\n",
    "3. **Missing Value Handling**:\n",
    "   - Nhi·ªÅu columns c√≥ t·ª∑ l·ªá missing cao (>30%)\n",
    "   - C·∫ßn xem x√©t strategy cho missing values\n",
    "\n",
    "### Khuy·∫øn ngh·ªã:\n",
    "\n",
    "1. **ƒêi·ªÅu ch·ªânh Price Validation**:\n",
    "   - Gi·∫£m minimum price threshold t·ª´ 100,000 xu·ªëng 10,000 VND\n",
    "   - Ho·∫∑c s·ª≠ d·ª•ng percentile-based filtering thay v√¨ hard thresholds\n",
    "\n",
    "2. **Improve Data Type Handling**:\n",
    "   - Ki·ªÉm tra v√† fix data type conversion logic\n",
    "   - Add more robust string-to-numeric conversion\n",
    "\n",
    "3. **Better Outlier Detection**:\n",
    "   - S·ª≠ d·ª•ng domain-specific knowledge v·ªÅ th·ªã tr∆∞·ªùng BDS\n",
    "   - Consider using percentile-based outlier detection\n",
    "\n",
    "4. **Data Quality Monitoring**:\n",
    "   - Add more detailed logging for each filtering step\n",
    "   - Create data quality reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133145ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"‚úÖ Investigation completed. Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5bff9-0ce8-415f-a163-e75064b71ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
