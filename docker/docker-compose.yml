# version: "3.8"

# Cấu hình chung sử dụng trên nhiều service
x-crawler-common: &crawler-common
  build:
    context: ../crawler
  restart: unless-stopped
  networks:
    - crawler_network
    - hdfs_network
  volumes:
    - ../crawler:/app
  depends_on:
    - kafka
    - namenode

x-airflow-common:
  &airflow-common
  image: apache/airflow:2.8.1
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
  volumes:
    - ../data_processing/airflow/dags:/opt/airflow/dags
    - ../data_processing/airflow/logs:/opt/airflow/logs
    - ../data_processing/airflow/plugins:/opt/airflow/plugins
    - /var/run/docker.sock:/var/run/docker.sock
  user: "${AIRFLOW_UID}:50000"
  networks:
    - airflow_network
    - hdfs_network
    - crawler_network

# Định nghĩa các service
services:
  # --- INFRASTRUCTURE SERVICES ---
  # ZooKeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    container_name: realestate_zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - crawler_network

  # Kafka - Message Queue
  kafka:
    image: confluentinc/cp-kafka:7.3.2
    container_name: realestate_kafka
    restart: unless-stopped
    ports:
      - "9092:9092"
    volumes:
      - ./volumes/kafka:/var/lib/kafka/data
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    networks:
      - crawler_network

  # HDFS NameNode
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: realestate_namenode
    restart: always
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - ./volumes/hdfs/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=realestate
    env_file:
      - ./hadoop.env
    networks:
      - hdfs_network

  # HDFS DataNodes
  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: realestate_datanode1
    restart: always
    volumes:
      - ./volumes/hdfs/datanode1:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hdfs_network

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: realestate_datanode2
    restart: always
    volumes:
      - ./volumes/hdfs/datanode2:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hdfs_network

  datanode3:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: realestate_datanode3
    restart: always
    volumes:
      - ./volumes/hdfs/datanode3:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hdfs_network

  # --- CRAWLER SERVICES ---
  # URL Crawler Service - Phát hiện URL mới và đẩy vào queue
  list-crawler-service:
    <<: *crawler-common
    container_name: realestate_list_crawler
    command: ["python", "-m", "services.list_crawler.main"]
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - HDFS_NAMENODE=namenode:9000
      - SOURCE=batdongsan
      - CRAWLER_TYPE=playwright
      - MAX_CONCURRENT=5

  # Detail Crawler Service - Xử lý chi tiết từ các URL
  detail-crawler-service:
    <<: *crawler-common
    container_name: realestate_detail_crawler
    command: ["python", "-m", "services.detail_crawler.main"]
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - HDFS_NAMENODE=namenode:9000
      - MAX_CONCURRENT=10
      - MAX_RETRIES=3

  # Storage Service - Lưu trữ dữ liệu vào HDFS
  storage-service:
    <<: *crawler-common
    container_name: realestate_storage
    command: ["python", "-m", "services.storage_service.main"]
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - HDFS_NAMENODE=namenode:9000
      - BATCH_SIZE=100
      - FLUSH_INTERVAL=300

  # Retry Service - Xử lý các URL thất bại
  retry-service:
    <<: *crawler-common
    container_name: realestate_retry
    command: ["python", "-m", "services.retry_service.main"]
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - MAX_RETRIES=3
      - RETRY_DELAY=3600

  # --- DATA PROCESSING SERVICES ---
  # Airflow - Orchestration
  postgres:
    image: postgres:13
    container_name: realestate_postgres
    restart: always
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./volumes/postgres:/var/lib/postgresql/data
    networks:
      - airflow_network

  airflow-init:
    <<: *airflow-common
    container_name: realestate_airflow_init
    entrypoint: bash
    command:
      - -c
      - |
        airflow db migrate && \
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin
    depends_on:
      - postgres

  airflow-webserver:
    <<: *airflow-common
    container_name: realestate_airflow_web
    restart: always
    depends_on:
      - postgres
    ports:
      - "8080:8080"
    command: webserver


  airflow-scheduler:
    <<: *airflow-common
    container_name: realestate_airflow_scheduler
    restart: always
    depends_on:
      - postgres
    command: scheduler

  # Spark - Data Processing
  spark-master:
    image: bitnami/spark:3.4.1
    container_name: realestate_spark_master
    environment:
      - SPARK_MODE=master
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - "7077:7077"
      - "8181:8080"  # Spark UI
    networks:
      - hdfs_network
      - spark_network

  spark-worker-1:
    image: bitnami/spark:3.4.1
    container_name: realestate_spark_worker1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    depends_on:
      - spark-master
    networks:
      - hdfs_network
      - spark_network

  spark-worker-2:
    image: bitnami/spark:3.4.1
    container_name: realestate_spark_worker2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    depends_on:
      - spark-master
    networks:
      - hdfs_network
      - spark_network

  # Jupyter - Data Analysis
  jupyter:
    build:
      context: ../data_processing
      dockerfile: notebooks/Dockerfile
    container_name: realestate_jupyter
    volumes:
      - ../data_processing/notebooks:/home/jovyan/work
    ports:
      - "8888:8888"
    networks:
      - hdfs_network
      - spark_network

  # --- APPLICATION SERVICES ---
  # API Service
  api:
    build: ../webapp/api_service
    container_name: realestate_api
    restart: always
    ports:
      - "8000:8000"
    volumes:
      - ../webapp/api_service:/app
    networks:
      - hdfs_network
      - app_network
    depends_on:
      - namenode

  # Web App
  webapp:
    build: ../webapp/client
    container_name: realestate_webapp
    restart: always
    ports:
      - "3000:80"  # Nginx serves on 80
    volumes:
      - ../webapp/client:/app
    networks:
      - app_network
    depends_on:
      - api

# Networks to isolate and connect services
networks:
  crawler_network:
    name: realestate_crawler_network
  hdfs_network:
    name: realestate_hdfs_network
  spark_network:
    name: realestate_spark_network
  airflow_network:
    name: realestate_airflow_network
  app_network:
    name: realestate_app_network
