# ğŸ” PhÃ¢n TÃ­ch Chi Tiáº¿t: Models TrÃªn HDFS & Kháº£ NÄƒng Truy Cáº­p

## ğŸ“‹ TÃ“M Táº®T

**CÃ‚U Há»I CHÃNH:**

1. Code `model_training.py` lÆ°u trá»¯ cÃ¡i gÃ¬ lÃªn HDFS chÃ­nh xÃ¡c?
2. Sau nÃ y cÃ³ thá»ƒ truy cáº­p vÃ o models Ä‘Ã£ lÆ°u trÃªn HDFS vÃ  dÃ¹ng Ä‘á»ƒ viáº¿t Python scripts dá»± Ä‘oÃ¡n giÃ¡ nhÃ  Ä‘Æ°á»£c khÃ´ng?

**CÃ‚U TRáº¢ Lá»œI NGáº®N Gá»ŒN:**
âœ… **CÃ“!** Há»‡ thá»‘ng lÆ°u trá»¯ Ä‘áº§y Ä‘á»§ models vÃ  metadata, hoÃ n toÃ n cÃ³ thá»ƒ viáº¿t scripts Python Ä‘á»ƒ load vÃ  sá»­ dá»¥ng.

---

## ğŸ—‚ï¸ PHáº¦N 1: Cáº¤U TRÃšC LÆ¯U TRá»® TRÃŠN HDFS

### ğŸ“ Structure Ä‘Æ°á»£c táº¡o bá»Ÿi `save_models()` method:

```
{model_output_path}/{property_type}/{date}/
â”œâ”€â”€ spark_models/                    # Spark ML Models
â”‚   â”œâ”€â”€ random_forest/              # RandomForest model files
â”‚   â”œâ”€â”€ gradient_boost/             # GBT model files
â”‚   â”œâ”€â”€ linear_regression/          # Linear regression files
â”‚   â””â”€â”€ decision_tree/              # Decision tree files
â”‚
â”œâ”€â”€ preprocessing_pipeline/          # Data preprocessing pipeline
â”‚   â”œâ”€â”€ stages/                     # Individual pipeline stages
â”‚   â””â”€â”€ metadata/                   # Pipeline metadata
â”‚
â”œâ”€â”€ advanced_models/                 # Sklearn-compatible models
â”‚   â”œâ”€â”€ xgboost_model.pkl          # XGBoost (joblib format)
â”‚   â”œâ”€â”€ lightgbm_model.pkl         # LightGBM (joblib format)
â”‚   â””â”€â”€ catboost_model.pkl         # CatBoost (joblib format)
â”‚
â””â”€â”€ model_registry.json             # Model metadata & metrics
```

### ğŸ“Š Chi tiáº¿t ná»™i dung tá»«ng thÃ nh pháº§n:

#### 1. **Spark Models** (spark_models/)

```python
# Má»—i model Ä‘Æ°á»£c lÆ°u vá»›i Spark's native format
for name, model in models.items():
    if model["model_type"] == "spark":
        spark_model_path = f"{model_dir}/spark_models/{name}"
        model["model"].write().overwrite().save(spark_model_path)
```

#### 2. **Preprocessing Pipeline** (preprocessing_pipeline/)

```python
# Pipeline xá»­ lÃ½ dá»¯ liá»‡u Ä‘áº§u vÃ o
preprocessing_path = f"{model_dir}/preprocessing_pipeline"
preprocessing_model.write().overwrite().save(preprocessing_path)
```

#### 3. **Advanced Models** (advanced_models/)

```python
# Sklearn-compatible models vá»›i joblib compression
for name, model in models.items():
    if model["model_type"] == "sklearn":
        model_file = f"{model_dir}/advanced_models/{name}_model.pkl"
        joblib.dump(model["model"], model_file, compress=3)
```

#### 4. **Model Registry** (model_registry.json)

```json
{
    "model_version": "v20250605_143022",
    "training_date": "2025-06-05",
    "timestamp": "20250605_143022",
    "best_model": {
        "name": "xgboost",
        "rmse": 25000000.0,
        "mae": 18000000.0,
        "r2": 0.85,
        "model_type": "sklearn"
    },
    "all_models": {
        "random_forest": { "rmse": 28000000.0, "mae": 20000000.0, "r2": 0.82 },
        "xgboost": { "rmse": 25000000.0, "mae": 18000000.0, "r2": 0.85 },
        "lightgbm": { "rmse": 26000000.0, "mae": 19000000.0, "r2": 0.84 }
    },
    "ensemble": {
        "models": ["xgboost", "lightgbm", "random_forest"],
        "weights": { "xgboost": 0.5, "lightgbm": 0.3, "random_forest": 0.2 }
    },
    "deployment_ready": true,
    "production_metrics": {
        "target_rmse_threshold": 27500000.0,
        "minimum_r2": 0.7,
        "passed_validation": true
    }
}
```

---

## ğŸ”§ PHáº¦N 2: CÃCH TRUY Cáº¬P & Sá»¬ Dá»¤NG MODELS

### ğŸš€ Táº¡o Model Loader Utility

Dá»±a trÃªn há»‡ thá»‘ng hiá»‡n táº¡i, báº¡n cÃ³ thá»ƒ táº¡o utility Ä‘á»ƒ load models:

```python
# utils/model_loader.py
import os
import json
import joblib
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel

class ModelLoader:
    def __init__(self, spark_session=None):
        self.spark = spark_session or SparkSession.builder.appName("ModelLoader").getOrCreate()

    def load_model_registry(self, model_path):
        """Load model registry Ä‘á»ƒ biáº¿t model nÃ o lÃ  best"""
        registry_path = f"{model_path}/model_registry.json"

        # Read JSON from Spark DataFrame format
        registry_df = self.spark.read.json(registry_path)
        registry_json = registry_df.collect()[0]['registry']
        return json.loads(registry_json)

    def load_best_model(self, model_path):
        """Load best model dá»±a trÃªn registry"""
        registry = self.load_model_registry(model_path)
        best_model_info = registry['best_model']

        if best_model_info['model_type'] == 'spark':
            # Load Spark model
            model_file = f"{model_path}/spark_models/{best_model_info['name']}"
            return PipelineModel.load(model_file)

        elif best_model_info['model_type'] == 'sklearn':
            # Load sklearn model
            model_file = f"{model_path}/advanced_models/{best_model_info['name']}_model.pkl"
            return joblib.load(model_file)

    def load_preprocessing_pipeline(self, model_path):
        """Load preprocessing pipeline"""
        pipeline_path = f"{model_path}/preprocessing_pipeline"
        return PipelineModel.load(pipeline_path)
```

### ğŸ¯ Script Dá»± ÄoÃ¡n GiÃ¡ NhÃ 

```python
# scripts/house_price_predictor.py
from utils.model_loader import ModelLoader
from pyspark.sql import SparkSession
import pandas as pd

class HousePricePredictor:
    def __init__(self, model_date="2025-06-05", property_type="house"):
        self.spark = SparkSession.builder.appName("HousePricePredictor").getOrCreate()
        self.loader = ModelLoader(self.spark)

        # ÄÆ°á»ng dáº«n model dá»±a trÃªn date
        self.model_path = f"/data/realestate/processed/ml/models/{property_type}/{model_date}"

        # Load models
        self.registry = self.loader.load_model_registry(self.model_path)
        self.preprocessing_pipeline = self.loader.load_preprocessing_pipeline(self.model_path)
        self.best_model = self.loader.load_best_model(self.model_path)

    def predict_single_house(self, house_data):
        """Dá»± Ä‘oÃ¡n giÃ¡ cho 1 cÄƒn nhÃ """
        # Convert to Spark DataFrame
        df = self.spark.createDataFrame([house_data])

        # Apply preprocessing pipeline
        processed_df = self.preprocessing_pipeline.transform(df)

        # Predict vá»›i best model
        if self.registry['best_model']['model_type'] == 'spark':
            predictions = self.best_model.transform(processed_df)
            return predictions.select("prediction").collect()[0]['prediction']

        else:  # sklearn model
            # Extract features vector
            features = processed_df.select("features").collect()[0]['features']
            # Convert Spark vector to numpy array
            feature_array = features.toArray().reshape(1, -1)
            return self.best_model.predict(feature_array)[0]

    def predict_batch(self, houses_data):
        """Dá»± Ä‘oÃ¡n giÃ¡ cho nhiá»u cÄƒn nhÃ """
        df = self.spark.createDataFrame(houses_data)
        processed_df = self.preprocessing_pipeline.transform(df)

        if self.registry['best_model']['model_type'] == 'spark':
            predictions = self.best_model.transform(processed_df)
            return predictions.select("id", "prediction").toPandas()
        else:
            # Handle sklearn batch prediction
            features_list = processed_df.select("features").collect()
            predictions = []
            for row in features_list:
                feature_array = row['features'].toArray().reshape(1, -1)
                pred = self.best_model.predict(feature_array)[0]
                predictions.append(pred)
            return predictions

# Sá»­ dá»¥ng:
predictor = HousePricePredictor(model_date="2025-06-05")

# Dá»± Ä‘oÃ¡n 1 cÄƒn nhÃ 
house = {
    "area_cleaned": 120.0,
    "bedrooms_cleaned": 3,
    "bathrooms_cleaned": 2,
    "city": "Ho Chi Minh City",
    "district": "District 1"
}

predicted_price = predictor.predict_single_house(house)
print(f"Predicted price: {predicted_price:,.0f} VND")
```

---

## ğŸ“ˆ PHáº¦N 3: KIá»‚M TRA THá»°C Táº¾

### ğŸ” Kiá»ƒm tra models hiá»‡n cÃ³:

```bash
# Kiá»ƒm tra structure trÃªn HDFS
hdfs dfs -ls /data/realestate/processed/ml/models/house/

# Xem chi tiáº¿t 1 model folder
hdfs dfs -ls /data/realestate/processed/ml/models/house/2025-06-05/
```

### ğŸ§ª Test model loading:

```python
# test_model_access.py
from utils.model_loader import ModelLoader

# Test load registry
loader = ModelLoader()
registry = loader.load_model_registry("/data/realestate/processed/ml/models/house/2025-06-05")
print("Best model:", registry['best_model']['name'])
print("RÂ²:", registry['best_model']['r2'])

# Test load actual model
model = loader.load_best_model("/data/realestate/processed/ml/models/house/2025-06-05")
print("Model loaded successfully:", type(model))
```

---

## âœ… PHáº¦N 4: Káº¾T LUáº¬N

### ğŸ¯ Tráº£ lá»i cÃ¢u há»i gá»‘c:

1. **Code lÆ°u gÃ¬ lÃªn HDFS?**

    - âœ… Spark ML models (native format)
    - âœ… Advanced models: XGBoost, LightGBM, CatBoost (joblib .pkl)
    - âœ… Preprocessing pipeline (Spark format)
    - âœ… Model registry vá»›i metrics vÃ  metadata (JSON)

2. **CÃ³ thá»ƒ viáº¿t Python scripts dá»± Ä‘oÃ¡n khÃ´ng?**
    - âœ… **CÃ“!** HoÃ n toÃ n cÃ³ thá»ƒ
    - âœ… Support cáº£ Spark vÃ  sklearn models
    - âœ… CÃ³ Ä‘áº§y Ä‘á»§ preprocessing pipeline
    - âœ… CÃ³ metadata Ä‘á»ƒ identify best model

### ğŸš€ Kiáº¿n trÃºc Ä‘Ã£ confirm:

```
Training Pipeline â†’ Save Best 4 Models to HDFS â†’ Prediction Server using saved models + PostgreSQL
```

**Status:** âœ… **FEASIBLE & READY TO IMPLEMENT**

### ğŸ“ Next Steps:

1. Táº¡o `ModelLoader` utility class
2. Implement prediction scripts
3. Set up Django API endpoints Ä‘á»ƒ serve predictions
4. Connect PostgreSQL cho data storage vÃ  logging
5. Deploy prediction server

Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ implement prediction service tá»« trained models trÃªn HDFS!
