# B·∫£ng Bi·ªÉu Tham S·ªë Container Thu Th·∫≠p D·ªØ Li·ªáu

## üìù T·ªïng quan h·ªá th·ªëng

H·ªá th·ªëng crawler bao g·ªìm 4 lo·∫°i service ch√≠nh:

1. **List Crawler** - Thu th·∫≠p danh s√°ch URL t·ª´ trang web
2. **Detail Crawler** - Thu th·∫≠p chi ti·∫øt t·ª´ URL ƒë√£ c√≥
3. **API Crawler** - Thu th·∫≠p d·ªØ li·ªáu qua API (ch·ªß y·∫øu cho Chotot)
4. **Storage Service** - L∆∞u tr·ªØ d·ªØ li·ªáu t·ª´ Kafka v√†o storage

### üèóÔ∏è Ki·∫øn tr√∫c h·ªó tr·ª£

**Ngu·ªìn d·ªØ li·ªáu h·ªó tr·ª£:**

-   `batdongsan` - batdongsan.com.vn (Playwright crawler)
-   `chotot` - chotot.com (API crawler + Detail crawler)

**Crawler types:**

-   `playwright` - S·ª≠ d·ª•ng Playwright browser automation
-   `default` - Crawler m·∫∑c ƒë·ªãnh cho t·ª´ng ngu·ªìn
-   `selenium` - S·ª≠ d·ª•ng Selenium (n·∫øu ƒë∆∞·ª£c implement)

---

## 1. List Crawler Container (Thu th·∫≠p danh s√°ch URL)

### Tham s·ªë m√¥i tr∆∞·ªùng (Environment Variables)

| Tham s·ªë                      | M√¥ t·∫£                                    | Ki·ªÉu d·ªØ li·ªáu | Gi√° tr·ªã m·∫∑c ƒë·ªãnh | V√≠ d·ª•                    | B·∫Øt bu·ªôc |
| ---------------------------- | ---------------------------------------- | ------------ | ---------------- | ------------------------ | -------- |
| `SOURCE`                     | Ngu·ªìn d·ªØ li·ªáu c·∫ßn thu th·∫≠p               | string       | "batdongsan"     | "batdongsan", "chotot"   | ‚úÖ       |
| `CRAWLER_TYPE`               | Lo·∫°i crawler engine                      | string       | "playwright"     | "playwright", "selenium" | ‚ùå       |
| `MAX_CONCURRENT`             | S·ªë thread ƒë·ªìng th·ªùi                      | integer      | 5                | 3, 5, 10                 | ‚ùå       |
| `START_PAGE`                 | Trang b·∫Øt ƒë·∫ßu thu th·∫≠p                   | integer      | 1                | 1, 10, 50                | ‚ùå       |
| `END_PAGE`                   | Trang k·∫øt th√∫c thu th·∫≠p                  | integer      | 500              | 100, 500, 1000           | ‚ùå       |
| `OUTPUT_FILE`                | File xu·∫•t d·ªØ li·ªáu (n·∫øu kh√¥ng d√πng Kafka) | string       | null             | "/data/urls.json"        | ‚ùå       |
| `CHECKPOINT_DIR`             | Th∆∞ m·ª•c l∆∞u checkpoint                   | string       | "./checkpoint"   | "/data/checkpoint"       | ‚ùå       |
| `FORCE_CRAWL`                | B·∫Øt bu·ªôc crawl l·∫°i                       | boolean      | false            | "true", "false"          | ‚ùå       |
| `FORCE_CRAWL_INTERVAL_HOURS` | Kho·∫£ng th·ªùi gian force crawl (gi·ªù)       | float        | 24               | 12, 24, 48               | ‚ùå       |

### Kafka Configuration

| Tham s·ªë                   | M√¥ t·∫£          | Gi√° tr·ªã m·∫∑c ƒë·ªãnh | V√≠ d·ª•                                          |
| ------------------------- | -------------- | ---------------- | ---------------------------------------------- |
| `KAFKA_BOOTSTRAP_SERVERS` | Kafka server   | "kafka1:19092"   | "kafka:9092", "kafka1:19092", "localhost:9092" |
| `KAFKA_TOPIC_URLS`        | Topic g·ª≠i URLs | "property-urls"  | "property-urls"                                |
| `KAFKA_CLIENT_ID`         | Client ID      | hostname         | "list-crawler-01"                              |

### V√≠ d·ª• Docker Run - List Crawler

#### C·∫•u h√¨nh c∆° b·∫£n:

```bash
docker run -d \
  --name list-crawler-batdongsan \
  --network real_estate_network \
  -e SOURCE=batdongsan \
  -e CRAWLER_TYPE=playwright \
  -e MAX_CONCURRENT=5 \
  -e START_PAGE=1 \
  -e END_PAGE=100 \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 \
  crawler:latest python services/list_crawler/main.py
```

#### C·∫•u h√¨nh n√¢ng cao v·ªõi checkpoint:

```bash
docker run -d \
  --name list-crawler-batdongsan-advanced \
  --network real_estate_network \
  -v /host/checkpoint:/app/checkpoint \
  -e SOURCE=batdongsan \
  -e CRAWLER_TYPE=playwright \
  -e MAX_CONCURRENT=10 \
  -e START_PAGE=1 \
  -e END_PAGE=1000 \
  -e FORCE_CRAWL=true \
  -e FORCE_CRAWL_INTERVAL_HOURS=12 \
  -e CHECKPOINT_DIR=/app/checkpoint \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 \
  crawler:latest python services/list_crawler/main.py
```

#### C·∫•u h√¨nh xu·∫•t file thay v√¨ Kafka:

```bash
docker run -d \
  --name list-crawler-file-output \
  -v /host/data:/app/data \
  -e SOURCE=batdongsan \
  -e START_PAGE=1 \
  -e END_PAGE=50 \
  -e OUTPUT_FILE=/app/data/urls.json \
  crawler:latest python services/list_crawler/main.py
```

---

## 2. Detail Crawler Container (Thu th·∫≠p chi ti·∫øt t·ª´ URL)

### Tham s·ªë m√¥i tr∆∞·ªùng (Environment Variables)

| Tham s·ªë            | M√¥ t·∫£                                | Ki·ªÉu d·ªØ li·ªáu | Gi√° tr·ªã m·∫∑c ƒë·ªãnh | V√≠ d·ª•                   | B·∫Øt bu·ªôc |
| ------------------ | ------------------------------------ | ------------ | ---------------- | ----------------------- | -------- |
| `SOURCE`           | Ngu·ªìn d·ªØ li·ªáu                        | string       | "batdongsan"     | "batdongsan", "chotot"  | ‚úÖ       |
| `CRAWLER_TYPE`     | Lo·∫°i crawler engine                  | string       | "default"        | "default", "playwright" | ‚ùå       |
| `MAX_CONCURRENT`   | S·ªë thread ƒë·ªìng th·ªùi                  | integer      | 5                | 3, 5, 10, 20            | ‚ùå       |
| `BATCH_SIZE`       | K√≠ch th∆∞·ªõc batch x·ª≠ l√Ω               | integer      | 100              | 50, 100, 200            | ‚ùå       |
| `RETRY_LIMIT`      | S·ªë l·∫ßn retry khi l·ªói                 | integer      | 3                | 2, 3, 5                 | ‚ùå       |
| `RETRY_DELAY`      | Th·ªùi gian ch·ªù gi·ªØa c√°c retry (gi√¢y)  | integer      | 5                | 3, 5, 10                | ‚ùå       |
| `OUTPUT_TOPIC`     | Kafka topic xu·∫•t d·ªØ li·ªáu             | string       | "property-data"  | "property-data"         | ‚ùå       |
| `RUN_ONCE_MODE`    | Ch·∫ø ƒë·ªô ch·∫°y m·ªôt l·∫ßn r·ªìi d·ª´ng         | boolean      | false            | "true", "false"         | ‚ùå       |
| `IDLE_TIMEOUT`     | Timeout khi kh√¥ng c√≥ d·ªØ li·ªáu (gi√¢y)  | integer      | 60               | 30, 60, 120             | ‚ùå       |
| `COMMIT_THRESHOLD` | S·ªë URL x·ª≠ l√Ω tr∆∞·ªõc khi commit offset | integer      | 20               | 10, 20, 50              | ‚ùå       |
| `COMMIT_INTERVAL`  | Th·ªùi gian gi·ªØa c√°c l·∫ßn commit (gi√¢y) | integer      | 60               | 30, 60, 120             | ‚ùå       |

### Kafka Configuration

| Tham s·ªë                      | M√¥ t·∫£                 | Gi√° tr·ªã m·∫∑c ƒë·ªãnh       | V√≠ d·ª•                                          |
| ---------------------------- | --------------------- | ---------------------- | ---------------------------------------------- |
| `KAFKA_BOOTSTRAP_SERVERS`    | Kafka server          | "kafka1:19092"         | "kafka:9092", "kafka1:19092", "localhost:9092" |
| `KAFKA_GROUP_ID`             | Consumer group ID     | "detail-crawler-group" | "detail-crawler-group"                         |
| `KAFKA_TOPIC_INPUT`          | Topic nh·∫≠n URLs       | "property-urls"        | "property-urls"                                |
| `KAFKA_TOPIC_OUTPUT`         | Topic g·ª≠i d·ªØ li·ªáu     | "property-data"        | "property-data"                                |
| `KAFKA_CLIENT_ID`            | Client ID             | hostname               | "detail-crawler-01"                            |
| `KAFKA_AUTO_OFFSET_RESET`    | Offset reset strategy | "earliest"             | "earliest", "latest"                           |
| `KAFKA_ENABLE_AUTO_COMMIT`   | Auto commit offset    | false                  | "true", "false"                                |
| `KAFKA_MAX_POLL_INTERVAL_MS` | Max poll interval     | 600000                 | 300000, 600000                                 |

### V√≠ d·ª• Docker Run - Detail Crawler

#### C·∫•u h√¨nh c∆° b·∫£n:

```bash
docker run -d \
  --name detail-crawler-batdongsan \
  --network real_estate_network \
  -e SOURCE=batdongsan \
  -e CRAWLER_TYPE=default \
  -e MAX_CONCURRENT=5 \
  -e BATCH_SIZE=100 \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 \
  crawler:latest python services/detail_crawler/main.py
```

#### C·∫•u h√¨nh hi·ªáu su·∫•t cao:

```bash
docker run -d \
  --name detail-crawler-high-performance \
  --network real_estate_network \
  --memory=2g \
  --cpus=2 \
  -e SOURCE=batdongsan \
  -e CRAWLER_TYPE=playwright \
  -e MAX_CONCURRENT=20 \
  -e BATCH_SIZE=200 \
  -e RETRY_LIMIT=2 \
  -e RETRY_DELAY=3 \
  -e COMMIT_THRESHOLD=50 \
  -e COMMIT_INTERVAL=30 \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 \
  crawler:latest python services/detail_crawler/main.py
```

#### C·∫•u h√¨nh ch·∫ø ƒë·ªô run-once:

```bash
docker run -d \
  --name detail-crawler-run-once \
  --network real_estate_network \
  -e SOURCE=batdongsan \
  -e RUN_ONCE_MODE=true \
  -e IDLE_TIMEOUT=120 \
  -e MAX_CONCURRENT=10 \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 \
  crawler:latest python services/detail_crawler/main.py
```

---

## 3. API Crawler Container (Thu th·∫≠p d·ªØ li·ªáu qua API - ch·ªß y·∫øu Chotot)

### Tham s·ªë m√¥i tr∆∞·ªùng (Environment Variables)

| Tham s·ªë           | M√¥ t·∫£                                     | Ki·ªÉu d·ªØ li·ªáu | Gi√° tr·ªã m·∫∑c ƒë·ªãnh | V√≠ d·ª•                       | B·∫Øt bu·ªôc |
| ----------------- | ----------------------------------------- | ------------ | ---------------- | --------------------------- | -------- |
| `SOURCE`          | Ngu·ªìn d·ªØ li·ªáu API                         | string       | "chotot"         | "chotot"                    | ‚úÖ       |
| `MAX_CONCURRENT`  | S·ªë thread ƒë·ªìng th·ªùi                       | integer      | 5                | 3, 5, 10                    | ‚ùå       |
| `START_PAGE`      | Trang b·∫Øt ƒë·∫ßu thu th·∫≠p                    | integer      | 1                | 1, 10, 50                   | ‚ùå       |
| `END_PAGE`        | Trang k·∫øt th√∫c thu th·∫≠p                   | integer      | 5                | 5, 10, 100                  | ‚ùå       |
| `REGION`          | Khu v·ª±c c·∫ßn thu th·∫≠p                      | string       | null             | "13000" (HCM), "12000" (HN) | ‚ùå       |
| `OUTPUT_TOPIC`    | Kafka topic xu·∫•t d·ªØ li·ªáu                  | string       | "property-data"  | "property-data"             | ‚ùå       |
| `INTERVAL`        | Kho·∫£ng th·ªùi gian gi·ªØa c√°c l·∫ßn ch·∫°y (gi√¢y) | integer      | 3600             | 1800, 3600, 7200            | ‚ùå       |
| `STOP_ON_EMPTY`   | D·ª´ng khi g·∫∑p trang tr·ªëng                  | boolean      | true             | "true", "false"             | ‚ùå       |
| `MAX_EMPTY_PAGES` | S·ªë trang tr·ªëng t·ªëi ƒëa tr∆∞·ªõc khi d·ª´ng      | integer      | 2                | 1, 2, 5                     | ‚ùå       |

### V√≠ d·ª• Docker Run - API Crawler

#### C·∫•u h√¨nh c∆° b·∫£n cho Chotot:

```bash
docker run -d \
  --name api-crawler-chotot \
  --network real_estate_network \
  -e SOURCE=chotot \
  -e MAX_CONCURRENT=5 \
  -e START_PAGE=1 \
  -e END_PAGE=50 \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 \
  crawler:latest python services/api_crawler/main.py
```

#### C·∫•u h√¨nh cho khu v·ª±c c·ª• th·ªÉ (HCM):

```bash
docker run -d \
  --name api-crawler-chotot-hcm \
  --network real_estate_network \
  -e SOURCE=chotot \
  -e REGION=13000 \
  -e MAX_CONCURRENT=10 \
  -e START_PAGE=1 \
  -e END_PAGE=100 \
  -e INTERVAL=1800 \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 \
  crawler:latest python services/api_crawler/main.py
```

---

## 4. Storage Service Container (L∆∞u tr·ªØ d·ªØ li·ªáu t·ª´ Kafka)

### Tham s·ªë m√¥i tr∆∞·ªùng (Environment Variables)

| Tham s·ªë          | M√¥ t·∫£                          | Ki·ªÉu d·ªØ li·ªáu | Gi√° tr·ªã m·∫∑c ƒë·ªãnh        | V√≠ d·ª•                          | B·∫Øt bu·ªôc |
| ---------------- | ------------------------------ | ------------ | ----------------------- | ------------------------------ | -------- |
| `STORAGE_TYPE`   | Lo·∫°i storage                   | string       | "local"                 | "local", "hdfs", "s3"          | ‚ùå       |
| `BATCH_SIZE`     | K√≠ch th∆∞·ªõc batch ƒë·ªÉ flush      | integer      | 20000                   | 10000, 20000, 50000            | ‚ùå       |
| `FLUSH_INTERVAL` | Th·ªùi gian flush ƒë·ªãnh k·ª≥ (gi√¢y) | integer      | 300                     | 180, 300, 600                  | ‚ùå       |
| `KAFKA_TOPIC`    | Kafka topic ƒë·ªçc d·ªØ li·ªáu        | string       | "property-data"         | "property-data"                | ‚ùå       |
| `KAFKA_GROUP_ID` | Consumer group ID              | string       | "storage-service-group" | "storage-service-group"        | ‚ùå       |
| `FILE_PREFIX`    | Prefix cho t√™n file            | string       | "property_data"         | "property_data", "real_estate" | ‚ùå       |
| `FILE_FORMAT`    | ƒê·ªãnh d·∫°ng file l∆∞u tr·ªØ         | string       | "json"/"parquet"        | "json", "parquet", "csv"       | ‚ùå       |

### Storage Type Specific Parameters

#### Local Storage:

| Tham s·ªë           | M√¥ t·∫£                 | Gi√° tr·ªã m·∫∑c ƒë·ªãnh |
| ----------------- | --------------------- | ---------------- |
| `LOCAL_BASE_PATH` | Th∆∞ m·ª•c l∆∞u tr·ªØ local | "/data"          |

#### HDFS Storage:

| Tham s·ªë             | M√¥ t·∫£                  | Gi√° tr·ªã m·∫∑c ƒë·ªãnh       |
| ------------------- | ---------------------- | ---------------------- |
| `HDFS_NAMENODE_URL` | URL c·ªßa HDFS namenode  | "hdfs://namenode:9000" |
| `HDFS_BASE_PATH`    | Th∆∞ m·ª•c base tr√™n HDFS | "/real_estate"         |

#### S3 Storage:

| Tham s·ªë                 | M√¥ t·∫£          | Gi√° tr·ªã m·∫∑c ƒë·ªãnh |
| ----------------------- | -------------- | ---------------- |
| `AWS_ACCESS_KEY_ID`     | AWS Access Key | -                |
| `AWS_SECRET_ACCESS_KEY` | AWS Secret Key | -                |
| `AWS_S3_BUCKET`         | S3 bucket name | -                |
| `AWS_S3_PREFIX`         | S3 prefix path | "real_estate/"   |

### V√≠ d·ª• Docker Run - Storage Service

#### Local storage:

```bash
docker run -d \
  --name storage-service-local \
  --network real_estate_network \
  -v /host/data:/data \
  -e STORAGE_TYPE=local \
  -e LOCAL_BASE_PATH=/data \
  -e BATCH_SIZE=10000 \
  -e FILE_FORMAT=parquet \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 \
  crawler:latest python services/storage_service/main.py
```

#### HDFS storage:

```bash
docker run -d \
  --name storage-service-hdfs \
  --network real_estate_network \
  -e STORAGE_TYPE=hdfs \
  -e HDFS_NAMENODE_URL=hdfs://namenode:9000 \
  -e HDFS_BASE_PATH=/real_estate \
  -e BATCH_SIZE=20000 \
  -e FILE_FORMAT=json \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 \
  crawler:latest python services/storage_service/main.py
```

---

## 5. Docker Compose Example - H·ªá th·ªëng ƒë·∫ßy ƒë·ªß

### C·∫•u h√¨nh ƒë·∫ßy ƒë·ªß h·ªá th·ªëng crawler v·ªõi t·∫•t c·∫£ services

```yaml
version: '3.8'

services:
    # Kafka Infrastructure
    zookeeper:
        image: confluentinc/cp-zookeeper:latest
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181
            ZOOKEEPER_TICK_TIME: 2000
        networks:
            - real_estate_network

    kafka:
        image: confluentinc/cp-kafka:latest
        depends_on:
            - zookeeper
        ports:
            - '9092:9092'
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:19092,PLAINTEXT_HOST://localhost:9092
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
            KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        networks:
            - real_estate_network

    # List Crawler cho Batdongsan
    list-crawler-batdongsan:
        build: ./crawler
        container_name: list-crawler-batdongsan
        environment:
            - SOURCE=batdongsan
            - CRAWLER_TYPE=playwright
            - MAX_CONCURRENT=5
            - START_PAGE=1
            - END_PAGE=500
            - FORCE_CRAWL=false
            - FORCE_CRAWL_INTERVAL_HOURS=24
            - KAFKA_BOOTSTRAP_SERVERS=kafka:19092
        volumes:
            - ./data/checkpoint:/app/checkpoint
        command: python services/list_crawler/main.py
        depends_on:
            - kafka
        networks:
            - real_estate_network

    # Detail Crawler cho Batdongsan
    detail-crawler-batdongsan:
        build: ./crawler
        container_name: detail-crawler-batdongsan
        environment:
            - SOURCE=batdongsan
            - CRAWLER_TYPE=default
            - MAX_CONCURRENT=10
            - BATCH_SIZE=100
            - RETRY_LIMIT=3
            - RETRY_DELAY=5
            - RUN_ONCE_MODE=false
            - KAFKA_BOOTSTRAP_SERVERS=kafka:19092
        command: python services/detail_crawler/main.py
        depends_on:
            - kafka
            - list-crawler-batdongsan
        networks:
            - real_estate_network

    # API Crawler cho Chotot
    api-crawler-chotot:
        build: ./crawler
        container_name: api-crawler-chotot
        environment:
            - SOURCE=chotot
            - MAX_CONCURRENT=10
            - START_PAGE=1
            - END_PAGE=100
            - INTERVAL=3600
            - STOP_ON_EMPTY=true
            - MAX_EMPTY_PAGES=2
            - KAFKA_BOOTSTRAP_SERVERS=kafka:19092
        command: python services/api_crawler/main.py
        depends_on:
            - kafka
        networks:
            - real_estate_network

    # Detail Crawler cho Chotot
    detail-crawler-chotot:
        build: ./crawler
        container_name: detail-crawler-chotot
        environment:
            - SOURCE=chotot
            - CRAWLER_TYPE=default
            - MAX_CONCURRENT=8
            - BATCH_SIZE=150
            - KAFKA_BOOTSTRAP_SERVERS=kafka:19092
        command: python services/detail_crawler/main.py
        depends_on:
            - kafka
            - api-crawler-chotot
        networks:
            - real_estate_network

    # Storage Service - Local
    storage-service-local:
        build: ./crawler
        container_name: storage-service-local
        environment:
            - STORAGE_TYPE=local
            - LOCAL_BASE_PATH=/data
            - BATCH_SIZE=10000
            - FLUSH_INTERVAL=300
            - FILE_FORMAT=parquet
            - KAFKA_BOOTSTRAP_SERVERS=kafka:19092
            - KAFKA_TOPIC=property-data
        volumes:
            - ./data/output:/data
        command: python services/storage_service/main.py
        depends_on:
            - kafka
        networks:
            - real_estate_network

    # Storage Service - HDFS (optional)
    storage-service-hdfs:
        build: ./crawler
        container_name: storage-service-hdfs
        environment:
            - STORAGE_TYPE=hdfs
            - HDFS_NAMENODE_URL=hdfs://namenode:9000
            - HDFS_BASE_PATH=/real_estate
            - BATCH_SIZE=20000
            - FLUSH_INTERVAL=600
            - FILE_FORMAT=json
            - KAFKA_BOOTSTRAP_SERVERS=kafka:19092
        command: python services/storage_service/main.py
        depends_on:
            - kafka
        networks:
            - real_estate_network
        profiles:
            - hdfs # Ch·ªâ ch·∫°y khi c√≥ profile hdfs

networks:
    real_estate_network:
        driver: bridge

volumes:
    kafka_data:
    zookeeper_data:
```

### Ch·∫°y h·ªá th·ªëng:

```bash
# Ch·∫°y h·ªá th·ªëng c∆° b·∫£n (local storage)
docker-compose up -d

# Ch·∫°y v·ªõi HDFS storage
docker-compose --profile hdfs up -d

# Scale detail crawlers
docker-compose up -d --scale detail-crawler-batdongsan=3

# Xem logs
docker-compose logs -f list-crawler-batdongsan
docker-compose logs -f detail-crawler-batdongsan
```

```yaml
version: '3.8'

services:
    # List Crawler cho Batdongsan
    list-crawler-batdongsan:
        build: ./crawler
        container_name: list-crawler-batdongsan
        environment:
            - SOURCE=batdongsan
            - CRAWLER_TYPE=playwright
            - MAX_CONCURRENT=5
            - START_PAGE=1
            - END_PAGE=500
            - FORCE_CRAWL=false
            - FORCE_CRAWL_INTERVAL_HOURS=24
            - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
        volumes:
            - ./data/checkpoint:/app/checkpoint
        command: python services/list_crawler/main.py
        depends_on:
            - kafka
        networks:
            - real_estate_network

    # Detail Crawler cho Batdongsan
    detail-crawler-batdongsan:
        build: ./crawler
        container_name: detail-crawler-batdongsan
        environment:
            - SOURCE=batdongsan
            - CRAWLER_TYPE=default
            - MAX_CONCURRENT=10
            - BATCH_SIZE=100
            - RETRY_LIMIT=3
            - RETRY_DELAY=5
            - RUN_ONCE_MODE=false
            - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
        command: python services/detail_crawler/main.py
        depends_on:
            - kafka
            - list-crawler-batdongsan
        networks:
            - real_estate_network

    # List Crawler cho Chotot
    list-crawler-chotot:
        build: ./crawler
        container_name: list-crawler-chotot
        environment:
            - SOURCE=chotot
            - CRAWLER_TYPE=playwright
            - MAX_CONCURRENT=3
            - START_PAGE=1
            - END_PAGE=200
            - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
        volumes:
            - ./data/checkpoint:/app/checkpoint
        command: python services/list_crawler/main.py
        depends_on:
            - kafka
        networks:
            - real_estate_network

    # Detail Crawler cho Chotot
    detail-crawler-chotot:
        build: ./crawler
        container_name: detail-crawler-chotot
        environment:
            - SOURCE=chotot
            - CRAWLER_TYPE=default
            - MAX_CONCURRENT=8
            - BATCH_SIZE=150
            - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
        command: python services/detail_crawler/main.py
        depends_on:
            - kafka
            - list-crawler-chotot
        networks:
            - real_estate_network

    # API Crawler cho Chotot
    api-crawler-chotot:
        build: ./crawler
        container_name: api-crawler-chotot
        environment:
            - SOURCE=chotot
            - MAX_CONCURRENT=5
            - START_PAGE=1
            - END_PAGE=50
            - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
        command: python services/api_crawler/main.py
        depends_on:
            - kafka
        networks:
            - real_estate_network

networks:
    real_estate_network:
        external: true
```

---

## 6. Advanced Configuration v√† Environment Variables ƒë·∫ßy ƒë·ªß

### Bi·∫øn m√¥i tr∆∞·ªùng chung cho t·∫•t c·∫£ services:

```bash
# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=kafka:19092
KAFKA_CLIENT_ID=crawler-service
KAFKA_AUTO_OFFSET_RESET=earliest
KAFKA_ENABLE_AUTO_COMMIT=false
KAFKA_MAX_POLL_INTERVAL_MS=600000

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Service Health Check
HEALTH_CHECK_INTERVAL=30
HEALTH_CHECK_TIMEOUT=10
```

### Factory Registry - C√°c lo·∫°i crawler c√≥ s·∫µn:

#### List Crawlers:

```python
LIST_CRAWLER_REGISTRY = {
    "batdongsan": {
        "default": BatdongsanListCrawler,
        "playwright": BatdongsanListCrawler,
    }
}
```

#### Detail Crawlers:

```python
DETAIL_CRAWLER_REGISTRY = {
    "batdongsan": {
        "default": BatdongsanDetailCrawler,
    },
    "chotot": {
        "default": ChototDetailCrawler,
    }
}
```

#### API Crawlers:

```python
API_CRAWLER_REGISTRY = {
    "chotot": {
        "default": ChototApiCrawler,
    }
}
```

---

## 7. Troubleshooting v√† Performance Tuning

### Memory v√† Resource Management:

```bash
# TƒÉng memory cho container
docker run --memory=4g --memory-swap=8g \
  --cpus=2.0 \
  -e MAX_CONCURRENT=20 \
  [other options...] crawler:latest

# Gi·ªõi h·∫°n JVM memory cho Kafka consumer
-e KAFKA_HEAP_OPTS="-Xmx2g -Xms1g"
```

### Kafka Performance Tuning:

```bash
# Producer settings
-e KAFKA_BATCH_SIZE=65536
-e KAFKA_LINGER_MS=10
-e KAFKA_COMPRESSION_TYPE=snappy

# Consumer settings
-e KAFKA_FETCH_MIN_BYTES=1024
-e KAFKA_FETCH_MAX_WAIT_MS=500
-e KAFKA_MAX_PARTITION_FETCH_BYTES=1048576
```

### Monitoring Commands:

```bash
# Ki·ªÉm tra Kafka topics v√† messages
docker exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092
docker exec kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic property-urls --from-beginning

# Monitor container resources
docker stats --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}"

# Check logs v·ªõi filters
docker logs -f --since=1h list-crawler-batdongsan | grep ERROR
docker logs -f detail-crawler-batdongsan | jq '.level' | sort | uniq -c
```

### Debug Mode:

```bash
# Ch·∫°y v·ªõi debug mode
docker run -it --rm \
  -e LOG_LEVEL=DEBUG \
  -e MAX_CONCURRENT=1 \
  -e START_PAGE=1 \
  -e END_PAGE=2 \
  crawler:latest python services/list_crawler/main.py
```

---

## 8. Security v√† Best Practices

### Security Configuration:

```bash
# Ch·∫°y v·ªõi non-root user
docker run --user 1000:1000 [options...] crawler:latest

# Network isolation
docker network create --driver bridge --internal crawler_internal_network

# Secrets management
docker run -e KAFKA_PASSWORD_FILE=/run/secrets/kafka_password \
  --secret kafka_password \
  [options...] crawler:latest
```

### Production Best Practices:

1. **Health Checks:**

```dockerfile
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s \
  CMD python -c "import requests; requests.get('http://localhost:8080/health')"
```

2. **Resource Limits:**

```yaml
deploy:
    resources:
        limits:
            cpus: '2.0'
            memory: 4G
        reservations:
            cpus: '0.5'
            memory: 1G
```

3. **Restart Policies:**

```yaml
restart: unless-stopped
deploy:
    restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
```

B·∫£ng bi·ªÉu n√†y b√¢y gi·ªù ƒë√£ bao g·ªìm ƒë·∫ßy ƒë·ªß th√¥ng tin v·ªÅ t·∫•t c·∫£ c√°c services trong h·ªá th·ªëng crawler, bao g·ªìm API Crawler v√† Storage Service m√† tr∆∞·ªõc ƒë√≥ ƒë√£ b·ªã thi·∫øu!
